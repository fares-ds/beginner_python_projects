,0
https://en.wikipedia.org/wiki/Machine_Learning_(journal),"Machine Learning  is a peer-reviewed scientific journal, published since 1986.
It should be distinguished from the journal Machine intelligence which was established in the mid-1960s.[1]
In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[2]
Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.[3]

"
https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition,"Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.
The earliest evidence for these statistical learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable “pseudowords” that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing “pseudowords” as opposed to “nonwords” from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words.[citation needed]
Since the initial discovery of the role of statistical learning in lexical acquisition, the same mechanism has been proposed for elements of phonological acquisition,  and syntactical acquisition, as well as in non-linguistic domains. Further research has also indicated that statistical learning is likely a domain-general and even species-general learning mechanism, occurring for visual as well as auditory information, and in both primates and non-primates.
The role of statistical learning in language acquisition has been particularly well documented in the area of lexical acquisition.[1] One important contribution to infants' understanding of segmenting words from a continuous stream of speech is their ability to recognize statistical regularities of the speech heard in their environments.[1] Although many factors play an important role, this specific mechanism is powerful and can operate over a short time scale.[1]
It is a well-established finding that, unlike written language, spoken language does not have any clear boundaries between words; spoken language is a continuous stream of sound rather than individual words with silences between them.[2] This lack of segmentation between linguistic units presents a problem for young children learning language, who must be able to pick out individual units from the continuous speech streams that they hear.[3] One proposed method of how children are able to solve this problem is that they are attentive to the statistical regularities of the world around them.[2][3] For example, in the phrase ""pretty baby,"" children are more likely to hear the sounds pre and ty heard together during the entirety of the lexical input around them than they are to hear the sounds ty and ba together.[3] In an artificial grammar learning study with adult participants, Saffran, Newport, and Aslin found that participants were able to locate word boundaries based only on transitional probabilities, suggesting that adults are capable of using statistical regularities in a language-learning task.[4] This is a robust finding that has been widely replicated.[1]
To determine if young children have these same abilities Saffran Aslin and Newport exposed 8-month-old infants to an artificial grammar.[3] The grammar was composed of four words, each composed of three nonsense syllables. During the experiment, infants heard a continuous speech stream of these words . Importantly, the speech was presented in a monotone with no cues (such as pauses, intonation, etc.) to word boundaries other than the statistical probabilities. Within a word, the transitional probability of two syllable pairs was 1.0: in the word bidaku, for example, the probability of hearing the syllable da immediately after the syllable bi was 100%. Between words, however, the transitional probability of hearing a syllable pair was much lower: After any given word (e.g., bidaku) was presented, one of three words could follow (in this case, padoti, golabu, or tupiro), so the likelihood of hearing any given syllable after ku was only 33%.
To determine if infants were picking up on the statistical information, each infant was presented with multiple presentations of either a word from the artificial grammar or a nonword made up of the same syllables but presented in a random order. Infants who were presented with nonwords during the test phase listened significantly longer to these words than infants who were presented with words from the artificial grammar, showing a novelty preference for these new nonwords. However, the implementation of the test could also be due to infants learning serial-order information and not to actually learning transitional probabilities between words. That is, at test, infants heard strings such as dapiku and tilado that were never presented during learning; they could simply have learned that the syllable ku never followed the syllable pi.[3]
To look more closely at this issue, Saffran Aslin and Newport conducted another study in which infants underwent the same training with the artificial grammar but then were presented with either words or part-words rather than words or nonwords.[3] The part-words were syllable sequences composed of the last syllable from one word and the first two syllables from another (such as kupado). Because the part-words had been heard during the time when children were listening to the artificial grammar, preferential listening to these part-words would indicate that children were learning not only serial-order information, but also the statistical likelihood of hearing particular syllable sequences. Again, infants showed greater listening times to the novel (part-) words, indicating that 8-month-old infants were able to extract these statistical regularities from a continuous speech stream.
This result has been the impetus for much more research on the role of statistical learning in lexical acquisition and other areas (see [1]). In a follow-up to the original report,[3] Aslin, Saffran, and Newport found that even when words and part words occurred equally often in the speech stream, but with different transitional probabilities between syllables of words and part words, infants were still able to detect the statistical regularities and still preferred to listen to the novel part-words over the familiarized words.[5] This finding provides stronger evidence that infants are able to pick up transitional probabilities from the speech they hear, rather than just being aware of frequencies of individual syllable sequences.[1]
Another follow-up study examined the extent to which the statistical information learned during this type of artificial grammar learning feeds into knowledge that infants may already have about their native language.[6] Infants preferred to listen to words over part-words, whereas there was no significant difference in the nonsense frame condition. This finding suggests that even pre-linguistic infants are able to integrate the statistical cues they learn in a laboratory into their previously-acquired knowledge of a language.[1][6] In other words, once infants have acquired some linguistic knowledge, they incorporate newly acquired information into that previously-acquired learning.
A related finding indicates that slightly older infants can acquire both lexical and grammatical regularities from a single set of input,[7] suggesting that they are able to use outputs of one type of statistical learning (cues that lead to the discovery of word boundaries) as input to a second type (cues that lead to the discovery of syntactical regularities.[1][7] At test, 12-month-olds preferred to listen to sentences that had the same grammatical structure as the artificial language they had been tested on rather than sentences that had a different (ungrammatical) structure. Because learning grammatical regularities requires infants to be able to determine boundaries between individual words, this indicates that infants who are still quite young are able to acquire multiple levels of language knowledge (both lexical and syntactical) simultaneously, indicating that statistical learning is a powerful mechanism at play in language learning.[1][7]
Despite the large role that statistical learning appears to play in lexical acquisition, it is likely not the only mechanism by which infants learn to segment words. Statistical learning studies are generally conducted with artificial grammars that have no cues to word boundary information other than transitional probabilities between words.  Real speech, though, has many different types of cues to word boundaries, including prosodic and phonotactic information.[8]
Together, the findings from these studies of statistical learning in language acquisition indicate that statistical properties of the language are a strong cue in helping infants learn their first language.[1]
There is much evidence that statistical learning is an important component of both discovering which phonemes are important for a given language and which contrasts within phonemes are important.[9][10][11] Having this knowledge is important for aspects of both speech perception and speech production.
Since the discovery of infants’ statistical learning abilities in word learning, the same general mechanism has also been studied in other facets of language learning. For example, it is well-established that infants can discriminate between phonemes of many different languages but eventually become unable to discriminate between phonemes that do not appear in their native language;[12] however, it was not clear how this decrease in discriminatory ability came about. Maye et al. suggested that the mechanism responsible might be a statistical learning mechanism in which infants track the distributional regularities of the sounds in their native language.[12] To test this idea, Maye et al. exposed 6- and 8-month-old infants to a continuum of speech sounds that varied on the degree to which they were voiced. The distribution that the infants heard was either bimodal, with sounds from both ends of the voicing continuum heard most often, or unimodal, with sounds from the middle of the distribution heard most often. The results indicated that infants from both age groups were sensitive to the distribution of phonemes. At test, infants heard either non-alternating (repeated exemplars of tokens 3 or 6 from an 8-token continuum) or alternating (exemplars of tokens 1 and 8) exposures to specific phonemes on the continuum. Infants exposed to the bimodal distribution listened longer to the alternating trials than the non-alternating trials while there was no difference in listening times for infants exposed to the unimodal distribution. This finding indicates that infants exposed the bimodal distribution were better able to discriminate sounds from the two ends of the distribution than were infants in the unimodal condition, regardless of age. This type of statistical learning differs from that used in lexical acquisition, as it requires infants to track frequencies rather than transitional probabilities, and has been named “distributional learning.”[10]
Distributional learning has also been found to help infants contrast two phonemes that they initially have difficulty in discriminating between. Maye, Weiss, and Aslin found that infants who were exposed to a bimodal distribution of a non-native contrast that was initially difficult to discriminate were better able to discriminate the contrast than infants exposed to a unimodal distribution of the same contrast.[13] Maye et al. also found that infants were able to abstract features of a contrast (i.e., voicing onset time) and generalize that feature to the same type of contrast at a different place of articulation, a finding that has not been found in adults.
In a review of the role of distributional learning on phonological acquisition, Werker et al. note that distributional learning cannot be the only mechanism by which phonetic categories are acquired.[10] However, it does seem clear that this type of statistical learning mechanism can play a role in this skill, although research is ongoing.[10]
A related finding regarding statistical cues to phonological acquisition is a phenomenon known as the perceptual magnet effect.[14][15][16] In this effect, a prototypical phoneme of a person's native language acts as a “magnet” for similar phonemes, which are perceived as belonging to the same category as the prototypical phoneme. In the original test of this effect, adult participants were asked to indicate if a given exemplar of a particular phoneme differed from a referent phoneme.[14] If the referent phoneme is a non-prototypical phoneme for that language, both adults and 6-month-old infants show less generalization to other sounds than they do for prototypical phonemes, even if the subjective distance between the sounds is the same.[14][16] That is, adults and infants are both more likely to notice that a particular phoneme differs from the referent phoneme if that referent phoneme is a non-prototypical exemplar than if it is a prototypical exemplar. The prototypes themselves are apparently discovered through a distributional learning process, in which infants are sensitive to the frequencies with which certain sounds occur and treat those that occur most often as the prototypical phonemes of their language.[11]
A statistical learning device has also been proposed as a component of syntactical acquisition for young children.[1][9][17] Early evidence for this mechanism came largely from studies of computer modeling or analyses of natural language corpora.[18][19] These early studies focused largely on distributional information specifically rather than statistical learning mechanisms generally. Specifically, in these early papers it was proposed that children created templates of possible sentence structures involving unnamed categories of word types (i.e., nouns or verbs, although children would not put these labels on their categories). Children were thought to learn which words belonged to the same categories by tracking the similar contexts in which words of the same category appeared.
Later studies expanded these results by looking at the actual behavior of children or adults who had been exposed to artificial grammars.[9] These later studies also considered the role of statistical learning more broadly than the earlier studies, placing their results in the context of the statistical learning mechanisms thought to be involved with other aspects of language learning, such as lexical acquisition.
Evidence from a series of four experiments conducted by Gomez and Gerken suggests that children are able to generalize grammatical structures with less than two minutes of exposure to an artificial grammar.[9][20] In the first experiment, 11-12 month-old infants were trained on an artificial grammar composed of nonsense words with a set grammatical structure. At test, infants heard both novel grammatical and ungrammatical sentences. Infants oriented longer towards the grammatical sentences, in line with previous research that suggests that infants generally orient for a longer amount of time to natural instances of language rather than altered instances of language e.g.,.[21] (This familiarity preference differs from the novelty preference generally found in word-learning studies, due to the differences between lexical acquisition and syntactical acquisition.) This finding indicates that young children are sensitive to the grammatical structure of language even after minimal exposure. Gomez and Gerken also found that this sensitivity is evident when ungrammatical transitions are located in the middle of the sentence (unlike in the first experiment, in which all the errors occurred at the beginning and end of the sentences), that the results could not be due to an innate preference for the grammatical sentences caused by something other than grammar, and that children are able to generalize the grammatical rules to new vocabulary.
Together these studies suggest that infants are able to extract a substantial amount of syntactic knowledge even from limited exposure to a language.[9][20] Children apparently detected grammatical anomalies whether the grammatical violation in the test sentences occurred at the end or in the middle of the sentence. Additionally, even when the individual words of the grammar were changed, infants were still able to discriminate between grammatical and ungrammatical strings during the test phase. This generalization indicates that infants were not learning vocabulary-specific grammatical structures, but abstracting the general rules of that grammar and applying those rules to novel vocabulary. Furthermore, in all four experiments, the test of grammatical structures occurred five minutes after the initial exposure to the artificial grammar had ended, suggesting that the infants were able to maintain the grammatical abstractions they had learned even after a short delay.
In a similar study, Saffran found that adults and older children (first and second grade children) were also sensitive to syntactical information after exposure to an artificial language which had no cues to phrase structure other than the statistical regularities that were present.[22] Both adults and children were able to pick out sentences that were ungrammatical at a rate greater than chance, even under an “incidental” exposure condition in which participants’ primary goal was to complete a different task while hearing the language.
Although the number of studies dealing with statistical learning of syntactical information is limited, the available evidence does indicate that the statistical learning mechanisms are likely a contributing factor to children's ability to learn their language.[9][17]
Much of the early work using statistical learning paradigms focused on the ability for children or adults to learn a single language,[1] consistent with the process of language acquisition for monolingual speakers or learners. However, it is estimated that approximately 60-75% of people in the world are bilingual.[23] More recently, researchers have begun looking at the role of statistical learning for those who speak more than one language. Although there are no reviews on this topic yet, Weiss, Gerfen, and Mitchel examined how hearing input from multiple artificial languages simultaneously can affect the ability to learn either or both languages.[24] Over four experiments, Weiss et al. found that, after exposure to two artificial languages, adult learners are capable of determining word boundaries in both languages when each language is spoken by a different speaker. However, when the two languages were spoken by the same speaker, participants were able learn both languages only when they were “congruent”—when the word boundaries of one language matched the word boundaries of the other. When the languages were incongruent—a syllable that appeared in the middle of a word in one language appeared at the end of the word in the other language—and spoken by a single speaker, participants were able to learn, at best, one of the two languages. A final experiment showed that the inability to learn incongruent languages spoken in the same voice was not due to syllable overlap between the languages but due to differing word boundaries.
Similar work replicates the finding that learners are able to learn two sets of statistical representations when an additional cue is present (two different male voices in this case).[25] In their paradigm, the two languages were presented consecutively, rather than interleaved as in Weiss et al.’s paradigm,[24] and participants did learn the first artificial language to which they had been exposed better than the second, although participants’ performance was above chance for both languages.
While statistical learning improves and strengthens multilingualism, it appears that the inverse is not true. In a study by Yim and Rudoy[26] it was found that both monolingual and bilingual children perform statistical learning tasks equally well.
Antovich and Graf Estes [27] found that 14 month old bilingual children are better than monolinguals at segmenting two different artificial languages using transitional probability cues. They suggest that a bilingual environment in early childhood trains children to rely on statistical regularities to segment the speech flow and access two lexical systems.
A statistical learning mechanism has also been proposed for learning the meaning of words. Specifically, Yu and Smith conducted a pair of studies in which adults were exposed to pictures of objects and heard nonsense words.[28] Each nonsense word was paired with a particular object. There were 18 total word-referent pairs, and each participant was presented with either 2, 3, or 4 objects at a time, depending on the condition, and heard the nonsense word associated with one of those objects. Each word-referent pair was presented 6 times over the course of the training trials; after the completion of the training trials, participants completed a forced-alternative test in which they were asked to choose the correct referent that matched a nonsense word they were given. Participants were able to choose the correct item more often than would happen by chance, indicating, according to the authors, that they were using statistical learning mechanisms to track co-occurrence probabilities across training trials.
An alternative hypothesis is that learners in this type of task may be using a “propose-but-verify” mechanism rather than a statistical learning mechanism.[29][30] Medina et al. and Trueswell et al. argue that, because Yu and Smith only tracked knowledge at the end of the training, rather than tracking knowledge on a trial-by-trial basis, it is impossible to know if participants were truly updating statistical probabilities of co-occurrence (and therefore maintaining multiple hypotheses simultaneously), or if, instead, they were forming a single hypothesis and checking it on the next trial.[28][29][30] For example, if a participant is presented with a picture of a dog and a picture of a shoe, and hears the nonsense word vash she might hypothesize that vash refers to the dog. On a future trial, she may see a picture of a shoe and a picture of a door and again hear the word vash. If statistical learning is the mechanism by which word-referent mappings are learned, then the participant would be more likely to select the picture of the shoe than the door, as shoe would have appeared in conjunction with the word vash 100% of the time. However, if participants are simply forming a single hypothesis, they may fail to remember the context of the previous presentation of vash (especially if, as in the experimental conditions, there are multiple trials with other words in between the two presentations of vash) and therefore be at chance in this second trial. According to this proposed mechanism of word learning, if the participant had correctly guessed that vash referred to the shoe in the first trial, her hypothesis would be confirmed in the subsequent trial.
To distinguish between these two possibilities, Trueswell et al. conducted a series of experiments similar to those conducted by Yu and Smith except that participants were asked to indicate their choice of the word-referent mapping on each trial, and only a single object name was presented on each trial (with varying numbers of objects).[28][30] Participants would therefore have been at chance when they are forced to make a choice in their first trial. The results from the subsequent trials indicate that participants were not using a statistical learning mechanism in these experiments, but instead were using a propose-and-verify mechanism, holding only one potential hypothesis in mind at a time. Specifically, if participants had chosen an incorrect word-referent mapping in an initial presentation of a nonsense word (from a display of five possible choices), their likelihood of choosing the correct word-referent mapping in the next trial of that word was still at chance, or 20%. If, though, the participant had chosen the correct word-referent mapping on an initial presentation of a nonsense word, the likelihood of choosing the correct word-referent mapping on the subsequent presentation of that word was approximately 50%. These results were also replicated in a condition where participants were choosing between only two alternatives. These results suggest that participants did not remember the surrounding context of individual presentations and were therefore not using statistical cues to determine the word-referent mappings. Instead, participants make a hypothesis regarding a word-referent mapping and, on the next presentation of that word, either confirm or reject the hypothesis accordingly.
Overall, these results, along with similar results from Medina et al., indicate that word meanings may not be learned through a statistical learning mechanism in these experiments, which ask participants to hypothesize a mapping even on the first occurrence (i.e., not cross-situationally).[29] However, when the propose-but-verify mechanism has been compared to a statistical learning mechanism, the former was unable to reproduce individual learning trajectories nor fit as well as the latter.[31]
Additionally, statistical learning by itself cannot account even for those aspects of language acquisition for which it has been shown to play a large role. For example, Kuhl, Tsao, and Liu found that young English-learning infants who spent time in a laboratory session with a native Mandarin speaker were able to distinguish between phonemes that occur in Mandarin but not in English, unlike infants who were in a control condition.[32] Infants in this control condition came to the lab as often as infants in the experimental condition, but were exposed only to English; when tested at a later date, they were unable to distinguish the Mandarin phonemes. In a second experiment, the authors presented infants with audio or audiovisual recordings of Mandarin speakers and tested the infants’ ability to distinguish between the Mandarin phonemes. In this condition, infants failed to distinguish the foreign language phonemes. This finding indicates that social interaction is a necessary component of language learning and that, even if infants are presented with the raw data of hearing a language, they are unable to take advantage of the statistical cues present in that data if they are not also experiencing the social interaction.[11]
Although the phenomenon of statistical learning was first discovered in the context of language acquisition and there is much evidence of its role in that purpose, work since the original discovery has suggested that statistical learning may be a domain general skill and is likely not unique to humans.[3][33] For example, Saffran, Johnson, Aslin, and Newport found that both adults and infants were able to learn statistical probabilities of “words” created by playing different musical tones (i.e., participants heard the musical notes D, E, and F presented together during training and were able to recognize those notes as a unit at test as compared to three notes that had not been presented together).[34] In non-auditory domains, there is evidence that humans are able to learn statistical visual information whether that information is presented across space, e.g.,[35] or time, e.g.,.[36] Evidence of statistical learning has also been found in other primates, e.g.,[37] and some limited statistical learning abilities have been found even in non-primates like rats.[38] Together these findings suggest that statistical learning may be a generalized learning mechanism that happens to be utilized in language acquisition, rather than a mechanism that is unique to the human infant's ability to learn his or her language(s).
Further evidence for domain general statistical learning was suggested in a study run through the University of Cornell Department of Psychology concerning visual statistical learning in infancy.  Researchers in this study questioned whether domain generality of statistical learning in infancy would be seen using visual information. After first viewing images in statistically predictable patterns, infants were then exposed to the same familiar patterns in addition to novel sequences of the same identical stimulus components. Interest in the visuals was measured by the amount of time the child looked at the stimuli in which the researchers named “looking time.” All ages of infant participants showed more interest in the novel sequence relative to the familiar sequence. In demonstrating a preference for the novel sequences (which violated the transitional probability that defined the grouping of the original stimuli) the results of the study support the likelihood of domain general statistical learning in infancy.[39]
"
https://en.wikipedia.org/wiki/Timeline_of_machine_learning,"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.

"
https://en.wikipedia.org/wiki/Computational_learning_theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]
Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning.  In supervised learning, an algorithm is given samples that are labeled in some useful way.  For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible.  The algorithm takes these previously labeled samples and uses them to induce a classifier.  This classifier is a function that assigns labels to samples, including samples that have not been seen previously by the algorithm.  The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.
In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning.[citation needed] In
computational learning theory, a computation is considered feasible if it can be done in polynomial time.[citation needed] There are two kinds of time
complexity results:
Negative results often rely on commonly believed, but yet unproven assumptions,[citation needed] such as:
There are several different approaches to computational learning theory based on making different assumptions about the
inference principles used to generalize from limited data. This includes different definitions of probability (see frequency probability, Bayesian probability) and different assumptions on the generation of samples.[citation needed] The different approaches include:[citation needed]
While its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. For example, PAC theory inspired boosting, VC theory led to support vector machines, and Bayesian inference led to belief networks.
A description of some of these publications is given at important publications in machine learning.
"
https://en.wikipedia.org/wiki/Statistical_learning_theory,"Statistical learning theory is a framework for machine learning
drawing from the fields of statistics and functional analysis.[1][2] Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.
The goals of learning are understanding and prediction. Learning falls into many categories, including supervised learning, unsupervised learning, online learning, and reinforcement learning. From the perspective of statistical learning theory, supervised learning is best understood.[3] Supervised learning involves learning from a training set of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output, such that the learned function can be used to predict the output from future input.
Depending on the type of output, supervised learning problems are either problems of regression or problems of classification. If the output takes a continuous range of values, it is a regression problem. Using Ohm's Law as an example, a regression could be performed with voltage as input and current as an output. The regression would find the functional relationship between voltage and current to be 



R


{\displaystyle R}

, such that
Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In facial recognition, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture.
After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.
Take 



X


{\displaystyle X}

 to be the vector space of all possible inputs, and 



Y


{\displaystyle Y}

 to be
the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space 



Z
=
X
×
Y


{\displaystyle Z=X\times Y}

, i.e. there exists some unknown 



p
(
z
)
=
p
(



x
→



,
y
)


{\displaystyle p(z)=p({\vec {x}},y)}

. The training set is made up of 



n


{\displaystyle n}

 samples from this probability distribution, and is notated 
Every 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 is an input vector from the training data, and 




y

i




{\displaystyle y_{i}}


is the output that corresponds to it.
In this formalism, the inference problem consists of finding a function 



f
:
X
→
Y


{\displaystyle f:X\to Y}

 such that 



f
(



x
→



)
∼
y


{\displaystyle f({\vec {x}})\sim y}

. Let 





H




{\displaystyle {\mathcal {H}}}

 be a space of functions 



f
:
X
→
Y


{\displaystyle f:X\to Y}

 called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let 



V
(
f
(



x
→



)
,
y
)


{\displaystyle V(f({\vec {x}}),y)}

 be the loss function, a metric for the difference between the predicted value 



f
(



x
→



)


{\displaystyle f({\vec {x}})}

 and the actual value 



y


{\displaystyle y}

. The expected risk is defined to be
The target function, the best possible function 



f


{\displaystyle f}

 that can be
chosen, is given by the 



f


{\displaystyle f}

 that satisfies
Because the probability distribution 



p
(



x
→



,
y
)


{\displaystyle p({\vec {x}},y)}

 is unknown, a
proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the empirical risk
A learning algorithm that chooses the function 




f

S




{\displaystyle f_{S}}

 that minimizes
the empirical risk is called empirical risk minimization.
The choice of loss function is a determining factor on the function 




f

S




{\displaystyle f_{S}}

 that will be chosen by the learning algorithm. The loss function
also affects the convergence rate for an algorithm. It is important for the loss function to be convex.[4]
Different loss functions are used depending on whether the problem is
one of regression or one of classification.
The most common loss function for regression is the square loss function (also known as the L2-norm). This familiar loss function is used in Ordinary Least Squares regression. The form is:
The absolute value loss (also known as the L1-norm) is also sometimes used:
In some sense the 0-1 indicator function is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with 



Y
=
{
−
1
,
1
}


{\displaystyle Y=\{-1,1\}}

, this is:
where 



θ


{\displaystyle \theta }

 is the Heaviside step function.
In machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.
Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well.[5][6] Regularization can solve the overfitting problem and give
the problem stability.
Regularization can be accomplished by restricting the hypothesis space 





H




{\displaystyle {\mathcal {H}}}

. A common example would be restricting 





H




{\displaystyle {\mathcal {H}}}

 to linear functions: this can be seen as a reduction to the standard problem of linear regression. 





H




{\displaystyle {\mathcal {H}}}

 could also be restricted to polynomial of degree 



p


{\displaystyle p}

, exponentials, or bounded functions on L1. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.
One example of regularization is Tikhonov regularization. This consists of minimizing
where 



γ


{\displaystyle \gamma }

 is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.[7]
"
https://en.wikipedia.org/wiki/Supervised_learning,"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2]  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).
The parallel task in human and animal psychology is often referred to as concept learning.
In order to solve a given problem of supervised learning, one has to perform the following steps:
A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).
There are four major issues to consider in supervised learning:
A first issue is the tradeoff between bias and variance.[3]  Imagine that we have available several different, but equally good, training data sets.  A learning algorithm is biased for a particular input 



x


{\displaystyle x}

 if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for 



x


{\displaystyle x}

.  A learning algorithm has high variance for a particular input 



x


{\displaystyle x}

 if it predicts different output values when trained on different training sets.  The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[4]  Generally, there is a tradeoff between bias and variance.  A learning algorithm with low bias must be ""flexible"" so that it can fit the data well.  But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance.  A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
The second issue is the amount of training data available relative to the complexity of the ""true"" function (classifier or regression function).  If the true function is simple, then an ""inflexible"" learning algorithm with high bias and low variance will be able to learn it from a small amount of data.  But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn from a very large amount of training data and using a ""flexible"" learning algorithm with low bias and high variance.
A third issue is the dimensionality of the input space.  If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features.  This is because the many ""extra"" dimensions can confuse the learning algorithm and cause it to have high variance.  Hence, high input dimensional typically requires tuning the classifier to have low variance and high bias.  In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function.  In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones.  This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
A fourth issue is the degree of noise in the desired output values (the supervisory target variables).  If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples.  Attempting to fit the data too carefully leads to overfitting.  You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled ""corrupts"" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm.  There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[5][6]
Other factors to consider when choosing and applying a learning algorithm include the following:
When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation).  Tuning the performance of a learning algorithm can be very time-consuming.  Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
The most widely used learning algorithms are: 
Given a set of 



N


{\displaystyle N}

 training examples of the form 



{
(

x

1


,

y

1


)
,
.
.
.
,
(

x

N


,


y

N


)
}


{\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}}

 such that 




x

i




{\displaystyle x_{i}}

 is the feature vector of the i-th example and 




y

i




{\displaystyle y_{i}}

 is its label (i.e., class), a learning algorithm seeks a function 



g
:
X
→
Y


{\displaystyle g:X\to Y}

, where 



X


{\displaystyle X}

 is the input space and




Y


{\displaystyle Y}

 is the output space.  The function 



g


{\displaystyle g}

 is an element of some space of possible functions 



G


{\displaystyle G}

, usually called the hypothesis space.  It is sometimes convenient to
represent 



g


{\displaystyle g}

 using a scoring function 



f
:
X
×
Y
→

R



{\displaystyle f:X\times Y\to \mathbb {R} }

 such that 



g


{\displaystyle g}

 is defined as returning the 



y


{\displaystyle y}

 value that gives the highest score: 



g
(
x
)
=



arg
⁡
max

y



f
(
x
,
y
)


{\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)}

.  Let 



F


{\displaystyle F}

 denote the space of scoring functions.
Although 



G


{\displaystyle G}

 and 



F


{\displaystyle F}

 can be any space of functions, many learning algorithms are probabilistic models where 



g


{\displaystyle g}

 takes the form of a conditional probability model 



g
(
x
)
=
P
(
y

|

x
)


{\displaystyle g(x)=P(y|x)}

, or 



f


{\displaystyle f}

 takes the form of a joint probability model 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

.  For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.
There are two basic approaches to choosing 



f


{\displaystyle f}

 or 



g


{\displaystyle g}

: empirical risk minimization and structural risk minimization.[7]  Empirical risk minimization seeks the function that best fits the training data.  Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.
In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

.  In order to measure how well a function fits the training data, a loss function 



L
:
Y
×
Y
→


R


≥
0




{\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}}

 is defined.  For training example 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

, the loss of predicting the value 






y
^





{\displaystyle {\hat {y}}}

 is 



L
(

y

i


,



y
^



)


{\displaystyle L(y_{i},{\hat {y}})}

.
The risk 



R
(
g
)


{\displaystyle R(g)}

 of function 



g


{\displaystyle g}

 is defined as the expected loss of 



g


{\displaystyle g}

.  This can be estimated from the training data as
In empirical risk minimization, the supervised learning algorithm seeks the function 



g


{\displaystyle g}

 that minimizes 



R
(
g
)


{\displaystyle R(g)}

.  Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find 



g


{\displaystyle g}

.
When 



g


{\displaystyle g}

 is a conditional probability distribution 



P
(
y

|

x
)


{\displaystyle P(y|x)}

 and the loss function is the negative log likelihood: 



L
(
y
,



y
^



)
=
−
log
⁡
P
(
y

|

x
)


{\displaystyle L(y,{\hat {y}})=-\log P(y|x)}

, then empirical risk minimization is equivalent to maximum likelihood estimation.
When 



G


{\displaystyle G}

 contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization.  The learning algorithm is able
to memorize the training examples without generalizing well.  This is called overfitting.
Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization.  The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.
A wide variety of penalties have been employed that correspond to different definitions of complexity.  For example, consider the case where the function 



g


{\displaystyle g}

 is a linear function of the form
A popular regularization penalty is 




∑

j



β

j


2




{\displaystyle \sum _{j}\beta _{j}^{2}}

, which is the squared Euclidean norm of the weights, also known as the 




L

2




{\displaystyle L_{2}}

 norm.  Other norms include the 




L

1




{\displaystyle L_{1}}

 norm, 




∑

j



|


β

j



|



{\displaystyle \sum _{j}|\beta _{j}|}

, and the 




L

0




{\displaystyle L_{0}}

 norm, which is the number of non-zero  




β

j




{\displaystyle \beta _{j}}

s.  The penalty will be denoted by 



C
(
g
)


{\displaystyle C(g)}

.
The supervised learning optimization problem is to find the function 



g


{\displaystyle g}

 that minimizes
The parameter 



λ


{\displaystyle \lambda }

 controls the bias-variance tradeoff.  When 



λ
=
0


{\displaystyle \lambda =0}

, this gives empirical risk minimization with low bias and high variance.  When 



λ


{\displaystyle \lambda }

 is large, the learning algorithm will have high bias and low variance.  The value of 



λ


{\displaystyle \lambda }

 can be chosen empirically via cross validation.
The complexity penalty has a Bayesian interpretation as the negative log prior probability of 



g


{\displaystyle g}

, 



−
log
⁡
P
(
g
)


{\displaystyle -\log P(g)}

, in which case 



J
(
g
)


{\displaystyle J(g)}

 is the posterior probabability of 



g


{\displaystyle g}

.
The training methods described above are discriminative training methods, because they seek to find a function 



g


{\displaystyle g}

 that discriminates well between the different output values (see discriminative model).  For the special case where 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

 is a joint probability distribution and the loss function is the negative log likelihood 



−

∑

i


log
⁡
P
(

x

i


,

y

i


)
,


{\displaystyle -\sum _{i}\log P(x_{i},y_{i}),}

 a risk minimization algorithm is said to perform generative training, because 



f


{\displaystyle f}

 can be regarded as a generative model that explains how the data were generated.  Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms.  In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.
There are several ways in which the standard supervised learning problem can be generalized:
"
https://en.wikipedia.org/wiki/Unsupervised_learning,"Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs.[1] It forms one of the three main categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning, a related variant, makes use of supervised and unsupervised techniques.
Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.[2]  Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
A central application of unsupervised learning is in the field of density estimation in statistics,[3] though unsupervised learning encompasses many other domains involving summarizing and explaining data features.  It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution 




p

X


(
x


|


y
)


{\textstyle p_{X}(x\,|\,y)}

 conditioned on the label 



y


{\textstyle y}

 of input data; unsupervised learning intends to infer an a priori probability distribution 




p

X


(
x
)


{\textstyle p_{X}(x)}

.
Generative adversarial networks can also be used with supervised learning, though they can also be applied to unsupervised and reinforcement techniques.
Some of the most common algorithms used in unsupervised learning include: 1) Clustering (2) Anomaly detection (3) Neural Networks (4) Approaches for learning latent variable models.
Each approach again uses several methods as follow:

The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together.[6] In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons.[7] A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.
Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.[8]
One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.
In particular, the method of moments is shown to be effective in learning the parameters of latent variable models.[9]
Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[9]
The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.[9]
"
https://en.wikipedia.org/wiki/Cluster_analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932[1] and introduced to psychology by Joseph Zubin in 1938[2] and Robert Tryon in 1939[3] and famously used by Cattell beginning in 1943[4] for trait theory classification in personality psychology.
The notion of a ""cluster"" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms.[5] There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these ""cluster models"" is key to understanding the differences between the various algorithms. Typical cluster models include:
A ""clustering"" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:
There are also finer distinctions possible, for example:
As listed above, clustering algorithms can be categorized based on their cluster model. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.
There is no objectively ""correct"" clustering algorithm, but as it was noted, ""clustering is in the eye of the beholder.""[5] The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model.[5] For example, k-means cannot find non-convex clusters.[5]
Connectivity-based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect ""objects"" to form ""clusters"" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name ""hierarchical clustering"" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.
Connectivity-based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances), and UPGMA or WPGMA (""Unweighted or Weighted Pair Group Method with Arithmetic Mean"", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).
These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as ""chaining phenomenon"", in particular with single-linkage clustering). In the general case, the complexity is 





O


(

n

3


)


{\displaystyle {\mathcal {O}}(n^{3})}

 for agglomerative clustering and 





O


(

2

n
−
1


)


{\displaystyle {\mathcal {O}}(2^{n-1})}

 for divisive clustering,[7] which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity 





O


(

n

2


)


{\displaystyle {\mathcal {O}}(n^{2})}

) are known: SLINK[8] for single-linkage and CLINK[9] for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete[citation needed]. They did however provide inspiration for many later methods such as density based clustering.
Single-linkage on Gaussian data. At 35 clusters, the biggest cluster starts fragmenting into smaller parts, while before it was still connected to the second largest due to the single-link effect.
Single-linkage on density-based clusters. 20 clusters extracted, most of which contain single elements, since linkage clustering does not have a notion of ""noise"".
In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximate method is Lloyd's algorithm,[10] often just referred to as ""k-means algorithm"" (although another algorithm introduced this name). It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means).
Most k-means-type algorithms require the number of clusters – k – to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders of clusters (which is not surprising since the algorithm optimizes cluster centers, not cluster borders).
K-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based clustering, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.
k-means separates data into Voronoi cells, which assumes equal-sized clusters (not adequate here)
k-means cannot represent density-based clusters
Centroid-based clustering problems such as k-means and k-medoids are special cases of the uncapacitated, metric facility location problem, a canonical problem in the operations research and computational geometry communities. In a basic facility location problem (of which there are numerous variants that model more elaborate settings), the task is to find the best warehouse locations to optimally service a given set of consumers. One may view ""warehouses"" as cluster centroids and ""consumer locations"" as the data to be clustered. This makes it possible to apply the well-developed algorithmic solutions from the facility location literature to the presently considered centroid-based clustering problem.
The article facility location problem provides a more comprehensive discussion, and the subsection facility location problem#applications to clustering provides details regarding the transformation of which we write.
The clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.
While the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.
One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modeled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.
Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).
On Gaussian-distributed data, EM works well, since it uses Gaussians for modelling clusters
Density-based clusters cannot be modeled using Gaussian distributions
In density-based clustering,[11] clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas - that are required to separate clusters - are usually considered to be noise and border points.
The most popular[12] density based clustering method is DBSCAN.[13] In contrast to many newer methods, it features a well-defined cluster model called ""density-reachability"". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low – it requires a linear number of range queries on the database – and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS[14] is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter 



ε


{\displaystyle \varepsilon }

, and produces a hierarchical result related to that of linkage clustering. DeLi-Clu,[15] Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the 



ε


{\displaystyle \varepsilon }

 parameter entirely and offering performance improvements over OPTICS by using an R-tree index.
The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions – a common use case in artificial data – the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.
Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these ""density attractors"" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.[15]
Density-based clustering with DBSCAN.
DBSCAN assumes clusters of similar density, and may have problems separating nearby clusters
OPTICS is a DBSCAN variant, improving handling of different densities clusters
The grid-based technique is used for a multi-dimensional data set.[16] In this technique, we create a grid structure, and the comparison is performed on grids (also known as cells). The grid-based technique is fast and has low computational complexity. There are two types of grid-based clustering methods: STING and CLIQUE. Steps involved in grid-based clustering algorithm are:
In recent years, considerable effort has been put into improving the performance of existing algorithms.[17][18] Among them are CLARANS (Ng and Han, 1994),[19] and BIRCH (Zhang et al., 1996).[20] With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting ""clusters"" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering.
For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (""correlated"") subspace clusters that can be modeled by giving a correlation of their attributes.[21] Examples for such clustering algorithms are CLIQUE[22] and SUBCLU.[23]
Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC,[24] hierarchical subspace clustering and DiSH[25]) and correlation clustering (HiCO,[26] hierarchical correlation clustering, 4C[27] using ""correlation connectivity"" and ERiC[28] exploring hierarchical density-based correlation clusters).
Several different clustering systems based on mutual information have been proposed. One is Marina Meilă's variation of information metric;[29] another provides hierarchical clustering.[30] Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information.[31] Also belief propagation, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms.[32]
Evaluation (or ""validation"") of clustering results is as difficult as the clustering itself.[33] Popular approaches involve ""internal"" evaluation, where the clustering is summarized to a single quality score, ""external"" evaluation, where the clustering is compared to an existing ""ground truth"" classification, ""manual"" evaluation by a human expert, and ""indirect"" evaluation by evaluating the utility of the clustering in its intended application.[34]
Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective. For example, one could cluster the data set by the Silhouette coefficient; except that there is no known efficient algorithm for this. By using such an internal measure for evaluation, one rather compares the similarity of the optimization problems,[34] and not necessarily how useful the clustering is.
External evaluation has similar problems: if we have such ""ground truth"" labels, then we would not need to cluster; and in practical applications we usually do not have such labels. On the other hand, the labels only reflect one possible partitioning of the data set, which does not imply that there does not exist a different, and maybe even better, clustering.
Neither of these approaches can therefore ultimately judge the actual quality of a clustering, but this needs human evaluation,[34] which is highly subjective. Nevertheless, such statistics can be quite informative in identifying bad clusterings,[35] but one should not dismiss subjective human evaluation.[35]
When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.[36] Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.
Therefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another.[5] Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion.[5] For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.
More than a dozen of internal evaluation measures exist, usually based on the intuition that items in the same cluster should be more similar than items in different clusters.[37]:115–121 For example, the following methods can be used to assess the quality of clustering algorithms based on internal criterion:
In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation.[33] These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies.[39] Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result.[39] In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.[40]
A number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.[33]
As with internal evaluation, several external evaluation measures exist,[37]:125–129 for example:
To measure cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering. One way to do this is to compare the data against random data. On average, random data should not have clusters.
Cluster analysis has been used to cluster stocks into sectors.[59]
"
https://en.wikipedia.org/wiki/Semi-supervised_learning,"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).
Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
A set of 



l


{\displaystyle l}

 independently identically distributed examples 




x

1


,
…
,

x

l


∈
X


{\displaystyle x_{1},\dots ,x_{l}\in X}

 with corresponding labels 




y

1


,
…
,

y

l


∈
Y


{\displaystyle y_{1},\dots ,y_{l}\in Y}

 and 



u


{\displaystyle u}

 unlabeled examples 




x

l
+
1


,
…
,

x

l
+
u


∈
X


{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}

 are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning.[1] The goal of transductive learning is to infer the correct labels for the given unlabeled data 




x

l
+
1


,
…
,

x

l
+
u




{\displaystyle x_{l+1},\dots ,x_{l+u}}

 only. The goal of inductive learning is to infer the correct mapping from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

.
Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.
In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:[2]
Points that are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.
The data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.
The data lie approximately on a manifold of much lower dimension than the input space. In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.
The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,[3] and images of various facial expressions are controlled by a few muscles. In these cases distances and smoothness in the natural space of the generating problem, is superior to considering the space of all possible acoustic waves or images, respectively.
The heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning,[2] with examples of applications starting in the 1960s).[4]
The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s.[5] Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.[6]
Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images.[7]
Generative approaches to statistical learning first seek to estimate 



p
(
x

|

y
)


{\displaystyle p(x|y)}

,[disputed  – discuss] the distribution of data points belonging to each class. The probability 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 that a given point 



x


{\displaystyle x}

 has label 



y


{\displaystyle y}

 is then proportional to 



p
(
x

|

y
)
p
(
y
)


{\displaystyle p(x|y)p(y)}

 by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about 



p
(
x
)


{\displaystyle p(x)}

) or as an extension of unsupervised learning (clustering plus some labels).
Generative models assume that the distributions take some particular form 



p
(
x

|

y
,
θ
)


{\displaystyle p(x|y,\theta )}

 parameterized by the vector 



θ


{\displaystyle \theta }

. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.
[8] 
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.[6]
The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.
The parameterized joint distribution can be written as 



p
(
x
,
y

|

θ
)
=
p
(
y

|

θ
)
p
(
x

|

y
,
θ
)


{\displaystyle p(x,y|\theta )=p(y|\theta )p(x|y,\theta )}

 by using the chain rule. Each parameter vector 



θ


{\displaystyle \theta }

 is associated with a decision function 




f

θ


(
x
)
=


argmax
y


 
p
(
y

|

x
,
θ
)


{\displaystyle f_{\theta }(x)={\underset {y}{\operatorname {argmax} }}\ p(y|x,\theta )}

. 
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by 



λ


{\displaystyle \lambda }

:
Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss 



(
1
−
y
f
(
x
)

)

+




{\displaystyle (1-yf(x))_{+}}

 for labeled data, a loss function 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

 is introduced over the unlabeled data by letting 



y
=
sign
⁡

f
(
x
)



{\displaystyle y=\operatorname {sign} {f(x)}}

. TSVM then selects 




f

∗


(
x
)
=

h

∗


(
x
)
+
b


{\displaystyle f^{*}(x)=h^{*}(x)+b}

 from a reproducing kernel Hilbert space 





H




{\displaystyle {\mathcal {H}}}

 by minimizing the regularized empirical risk:
An exact solution is intractable due to the non-convex term 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

, so research focuses on useful approximations.[9]
Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).
Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its 



k


{\displaystyle k}

 nearest neighbors or to examples within some distance 



ϵ


{\displaystyle \epsilon }

. The weight 




W

i
j




{\displaystyle W_{ij}}

 of an edge between 




x

i




{\displaystyle x_{i}}

 and 




x

j




{\displaystyle x_{j}}

 is then set to 




e



−
‖

x

i


−

x

j



‖

2



ϵ





{\displaystyle e^{\frac {-\|x_{i}-x_{j}\|^{2}}{\epsilon }}}

.
Within the framework of manifold regularization,[10][11] the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes
where 





H




{\displaystyle {\mathcal {H}}}

 is a reproducing kernel Hilbert space and 





M




{\displaystyle {\mathcal {M}}}

 is the manifold on which the data lie. The regularization parameters 




λ

A




{\displaystyle \lambda _{A}}

 and 




λ

I




{\displaystyle \lambda _{I}}

 control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian 



L
=
D
−
W


{\displaystyle L=D-W}

 where 




D

i
i


=

∑

j
=
1


l
+
u



W

i
j




{\displaystyle D_{ii}=\sum _{j=1}^{l+u}W_{ij}}

 and 




f



{\displaystyle \mathbf {f} }

 the vector 



[
f
(

x

1


)
…
f
(

x

l
+
u


)
]


{\displaystyle [f(x_{1})\dots f(x_{l+u})]}

, we have
The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.
Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples 




x

1


,
…
,

x

l
+
u




{\displaystyle x_{1},\dots ,x_{l+u}}

 may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples.
Self-training is a wrapper method for semi-supervised learning.[12] First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.[13]
Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.[14]
Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data).[15] More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).
Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.[16] Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.[17][18]
"
https://en.wikipedia.org/wiki/Reinforcement_learning,"Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).[1]

The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques.[2] The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.Reinforcement learning, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
Basic reinforcement is modeled as a Markov decision process:
Rules are often stochastic. The observation typically involves the scalar, immediate reward associated with the last transition. In many works, the agent is assumed to observe the current environmental state (full observability). If not, the agent has partial observability. Sometimes the set of actions available to the agent is restricted (a zero balance cannot be reduced. For example, if the current value of the agent is 3 and the state transition reduces the value by 4, the transition will not be allowed).
A reinforcement learning agent interacts with its environment in discrete time steps. At each time t, the agent receives an observation 




o

t




{\displaystyle o_{t}}

, which typically includes the reward 




r

t




{\displaystyle r_{t}}

. It then chooses an action 




a

t




{\displaystyle a_{t}}

 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 




s

t
+
1




{\displaystyle s_{t+1}}

 and the reward 




r

t
+
1




{\displaystyle r_{t+1}}

 associated with the transition 



(

s

t


,

a

t


,

s

t
+
1


)


{\displaystyle (s_{t},a_{t},s_{t+1})}

 is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can (possibly randomly) choose any action as a function of the history.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers[3] and Go (AlphaGo).
Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.
The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997).[5]
Reinforcement learning requires clever exploration mechanisms. Randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is 



ϵ


{\displaystyle \epsilon }

-greedy, where 



0
<
ϵ
<
1


{\displaystyle 0<\epsilon <1}

 is a parameter controlling the amount of exploration vs. exploitation.  With probability 



1
−
ϵ


{\displaystyle 1-\epsilon }

, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 



ϵ


{\displaystyle \epsilon }

, exploration is chosen, and the action is chosen uniformly at random. 



ϵ


{\displaystyle \epsilon }

 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[6]
Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.
The agent's action selection is modeled as a map called policy:
The policy map gives the probability of taking action 



a


{\displaystyle a}

 when in state 



s


{\displaystyle s}

.[7]:61 There are also non-probabilistic policies.
Value function 




V

π


(
s
)


{\displaystyle V_{\pi }(s)}

 is defined as the expected return starting with state 



s


{\displaystyle s}

, i.e. 




s

0


=
s


{\displaystyle s_{0}=s}

, and successively following policy 



π


{\displaystyle \pi }

. Hence, roughly speaking, the value function estimates ""how good"" it is to be in a given state.[7]:60
where the random variable 



R


{\displaystyle R}

 denotes the return, and is defined as the sum of future discounted rewards[clarification needed]
where 




r

t




{\displaystyle r_{t}}

 is the reward at step 



t


{\displaystyle t}

, 



γ
∈
[
0
,
1
]


{\displaystyle \gamma \in [0,1]}

 is the discount-rate.
The algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.
The brute force approach entails two steps:
One problem with this is that the number of policies can be large, or even infinite. Another is that variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.
These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.
Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the ""current"" [on-policy] or the optimal [off-policy] one).
These methods rely on the theory of MDPs, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best expected return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.
To define optimality in a formal manner, define the value of a policy 



π


{\displaystyle \pi }

 by
where 



R


{\displaystyle R}

 stands for the return associated with following 



π


{\displaystyle \pi }

 from the initial state 



s


{\displaystyle s}

. Defining 




V

∗


(
s
)


{\displaystyle V^{*}(s)}

 as the maximum possible value of 




V

π


(
s
)


{\displaystyle V^{\pi }(s)}

, where 



π


{\displaystyle \pi }

 is allowed to change,
A policy that achieves these optimal values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return 




ρ

π




{\displaystyle \rho ^{\pi }}

, since 




ρ

π


=
E
[

V

π


(
S
)
]


{\displaystyle \rho ^{\pi }=E[V^{\pi }(S)]}

, where 



S


{\displaystyle S}

 is a state randomly sampled from the distribution 



μ


{\displaystyle \mu }

[clarification needed].
Although state-values suffice to define optimality, it is useful to define action-values. Given a state 



s


{\displaystyle s}

, an action 



a


{\displaystyle a}

 and a policy 



π


{\displaystyle \pi }

, the action-value of the pair 



(
s
,
a
)


{\displaystyle (s,a)}

 under 



π


{\displaystyle \pi }

 is defined by
where 



R


{\displaystyle R}

 now stands for the random return associated with first taking action 



a


{\displaystyle a}

 in state 



s


{\displaystyle s}

 and following 



π


{\displaystyle \pi }

, thereafter.
The theory of MDPs states that if 




π

∗




{\displaystyle \pi ^{*}}

 is an optimal policy, we act optimally (take the optimal action) by choosing the action from 




Q


π

∗




(
s
,
⋅
)


{\displaystyle Q^{\pi ^{*}}(s,\cdot )}

 with the highest value at each state, 



s


{\displaystyle s}

. The action-value function of such an optimal policy (




Q


π

∗






{\displaystyle Q^{\pi ^{*}}}

) is called the optimal action-value function and is commonly denoted by 




Q

∗




{\displaystyle Q^{*}}

. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions 




Q

k




{\displaystyle Q_{k}}

 (



k
=
0
,
1
,
2
,
…


{\displaystyle k=0,1,2,\ldots }

) that converge to 




Q

∗




{\displaystyle Q^{*}}

. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.
Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.
Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy 



π


{\displaystyle \pi }

, the goal is to compute the function values 




Q

π


(
s
,
a
)


{\displaystyle Q^{\pi }(s,a)}

 (or a good approximation to them) for all state-action pairs 



(
s
,
a
)


{\displaystyle (s,a)}

. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair 



(
s
,
a
)


{\displaystyle (s,a)}

 can be computed by averaging the sampled returns that originated from 



(
s
,
a
)


{\displaystyle (s,a)}

 over time.  Given sufficient time, this procedure can thus construct a precise estimate 



Q


{\displaystyle Q}

 of the action-value function 




Q

π




{\displaystyle Q^{\pi }}

. This finishes the description of the policy evaluation step.
In the policy improvement step, the next policy is obtained by computing a greedy policy with respect to 



Q


{\displaystyle Q}

: Given a state 



s


{\displaystyle s}

, this new policy returns an action that maximizes 



Q
(
s
,
⋅
)


{\displaystyle Q(s,\cdot )}

. In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.
Problems with this procedure include:
The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor critic methods belong to this category.
The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[8][9] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[10] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping 



ϕ


{\displaystyle \phi }

 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair 



(
s
,
a
)


{\displaystyle (s,a)}

 are obtained by linearly combining the components of 



ϕ
(
s
,
a
)


{\displaystyle \phi (s,a)}

 with some weights 



θ


{\displaystyle \theta }

:
The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[11]
The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency. Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called 



λ


{\displaystyle \lambda }

 parameter 



(
0
≤
λ
≤
1
)


{\displaystyle (0\leq \lambda \leq 1)}

 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.
An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector 



θ


{\displaystyle \theta }

, let 




π

θ




{\displaystyle \pi _{\theta }}

 denote the policy associated to 



θ


{\displaystyle \theta }

. Defining the performance function by
under mild conditions this function will be differentiable as a function of the parameter vector 



θ


{\displaystyle \theta }

. If the gradient of 



ρ


{\displaystyle \rho }

 was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[12] (which is known as the likelihood ratio method in the simulation-based optimization literature).[13] Policy search methods have been used in the robotics context.[14] Many policy search methods may get stuck in local optima (as they are based on local search).
A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[15]
Both the asymptotic and finite-sample behavior of most algorithms is well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of large MDPs is largely unexplored (except for the case of bandit problems).[clarification needed] Although finite-time performance bounds appeared for many algorithms, these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
Research topics include 
Multiagent or distributed reinforcement learning is a topic of interest. Applications are expanding.[19]
Reinforcement learning algorithms such as TD learning are under investigation as a model for dopamine-based learning in the brain. In this model, the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error. Reinforcement learning has been used as a part of the model for human skill learning, especially in relation to the interaction between implicit and explicit learning in skill acquisition (the first publication on this application was in 1995–1996).[20]
This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[21] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[22]
In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[23]
In apprenticeship learning, an expert demonstrates the target behavior. The system tries to recover the policy via observation.
"
https://en.wikipedia.org/wiki/Feature_learning,"In machine learning, feature learning or representation learning[1] is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
Feature learning can be either supervised or unsupervised.
Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:
Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).
Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique[6] applied dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier.
Neural networks are a family of learning algorithms that use a ""network"" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).
Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is Siamese networks.
Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.[7][8] Several approaches are introduced in the following.
K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, although suboptimal greedy algorithms have been developed.
K-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration.[3] It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has been used to train RBF networks[9]). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms.[10]
In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task.[3] K-means also improves performance in the domain of NLP, specifically for named-entity recognition;[11] there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings).[8]
Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.
PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.
PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order moments of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).
Local linear embedding (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000).[12][13] The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.
LLE consists of two major steps. The first step is for ""neighbor-preserving"", where each input data point Xi is reconstructed as a weighted sum of K nearest neighbor data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for ""dimension reduction,"" by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a least squares problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.
The reconstruction weights obtained in the first step capture the ""intrinsic geometric properties"" of a neighborhood in the input data.[13] It is assumed that original data lie on a smooth lower-dimensional manifold, and the ""intrinsic geometric properties"" captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.
Independent component analysis (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components.[14] The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution.
Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data.[15] Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation.[16]
The hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes.[17] These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation.
Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures.[3][18] An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables.[clarification needed] Such conditional independence facilitates computations.
An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hinton's contrastive divergence (CD) algorithm.[18]
In general training RBM by solving the maximization problem tends to result in non-sparse representations. Sparse RBM[19] was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant 



p


{\displaystyle p}

.
An autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov[18] where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria are satisfied.
"
https://en.wikipedia.org/wiki/Sparse_dictionary_learning,"Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation. 
One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP[1] or fast non-iterative algorithms[2] can be used to recover the signal. 
One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting.  
Given the input dataset 



X
=
[

x

1


,
.
.
.
,

x

K


]
,

x

i


∈


R


d




{\displaystyle X=[x_{1},...,x_{K}],x_{i}\in \mathbb {R} ^{d}}

 we wish to find a dictionary 




D

∈


R


d
×
n


:
D
=
[

d

1


,
.
.
.
,

d

n


]


{\displaystyle \mathbf {D} \in \mathbb {R} ^{d\times n}:D=[d_{1},...,d_{n}]}

 and a representation 



R
=
[

r

1


,
.
.
.
,

r

K


]
,

r

i


∈


R


n




{\displaystyle R=[r_{1},...,r_{K}],r_{i}\in \mathbb {R} ^{n}}

 such that both 



‖
X
−

D

R

‖

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}}

 is minimized and the representations 




r

i




{\displaystyle r_{i}}

 are sparse enough. This can be formulated as the following optimization problem:






argmin


D

∈


C


,

r

i


∈


R


n






∑

i
=
1


K


‖

x

i


−

D


r

i



‖

2


2


+
λ
‖

r

i



‖

0




{\displaystyle {\underset {\mathbf {D} \in {\mathcal {C}},r_{i}\in \mathbb {R} ^{n}}{\text{argmin}}}\sum _{i=1}^{K}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{0}}

, where 





C


≡
{

D

∈


R


d
×
n


:
‖

d

i



‖

2


≤
1


∀
i
=
1
,
.
.
.
,
n
}


{\displaystyle {\mathcal {C}}\equiv \{\mathbf {D} \in \mathbb {R} ^{d\times n}:\|d_{i}\|_{2}\leq 1\,\,\forall i=1,...,n\}}

, 



λ
>
0


{\displaystyle \lambda >0}








C




{\displaystyle {\mathcal {C}}}

 is required to constrain 




D



{\displaystyle \mathbf {D} }

 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of 




r

i




{\displaystyle r_{i}}

.



λ


{\displaystyle \lambda }

 controls the trade off between the sparsity and the minimization error.
The minimization problem above is not convex because of the ℓ0-""norm"" and solving this problem is NP-hard.[3] In some cases L1-norm is known to ensure sparsity[4] and so the above becomes a convex optimization problem with respect to each of the variables 




D



{\displaystyle \mathbf {D} }

 and 




R



{\displaystyle \mathbf {R} }

 when the other one is fixed, but it is not jointly convex in 



(

D

,

R

)


{\displaystyle (\mathbf {D} ,\mathbf {R} )}

.

The dictionary 




D



{\displaystyle \mathbf {D} }

 defined above can be ""undercomplete"" if 



n
<
d


{\displaystyle n<d}

 or ""overcomplete"" in case 



n
>
d


{\displaystyle n>d}

 with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.
Undercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms 




d

1


,
.
.
.
,

d

n




{\displaystyle d_{1},...,d_{n}}

 to be orthogonal.   The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial.  And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.
Overcomplete dictionaries, however, do not require the atoms to be orthogonal (they will never be a basis anyway) thus allowing for more flexible dictionaries and richer data representations. 
An overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents the given signal in a best way. Learned dictionaries are capable of giving sparser solutions as compared to predefined transform matrices.
As the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.
The problem of finding an optimal sparse coding 



R


{\displaystyle R}

 with a given dictionary 




D



{\displaystyle \mathbf {D} }

 is known as sparse approximation (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as matching pursuit and LASSO) which are incorporated into the algorithms described below.
The method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem.[5] The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:





min


D

,
R


{
‖
X
−

D

R

‖

F


2


}



s.t.



∀
i


‖

r

i



‖

0


≤
T


{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T}


Here, 



F


{\displaystyle F}

 denotes the Frobenius norm. MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by 




D

=
X

R

+




{\displaystyle \mathbf {D} =XR^{+}}

 where 




R

+




{\displaystyle R^{+}}

 is a Moore-Penrose pseudoinverse. After this update 




D



{\displaystyle \mathbf {D} }

 is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).
MOD has proved to be a very efficient method for low-dimensional input data 



X


{\displaystyle X}

 requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.
K-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means. It enforces that each element of the input data 




x

i




{\displaystyle x_{i}}

 is encoded by a linear combination of not more than 




T

0




{\displaystyle T_{0}}

 elements in a way identical to the MOD approach:





min


D

,
R


{
‖
X
−

D

R

‖

F


2


}



s.t.



∀
i


‖

r

i



‖

0


≤

T

0




{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T_{0}}


This algorithm's essence is to first fix the dictionary, find the best possible 



R


{\displaystyle R}

 under the above constraint (using Orthogonal Matching Pursuit) and then iteratively update the atoms of dictionary 




D



{\displaystyle \mathbf {D} }

 in the following manner:




‖
X
−

D

R

‖

F


2


=


|

X
−

∑

i
=
1


K



d

i



x

T


i



|


F


2


=
‖

E

k


−

d

k



x

T


k



‖

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}=\left|X-\sum _{i=1}^{K}d_{i}x_{T}^{i}\right|_{F}^{2}=\|E_{k}-d_{k}x_{T}^{k}\|_{F}^{2}}


The next steps of the algorithm include rank-1 approximation of the residual matrix 




E

k




{\displaystyle E_{k}}

, updating 




d

k




{\displaystyle d_{k}}

 and enforcing the sparsity of 




x

k




{\displaystyle x_{k}}

 after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.
One can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem.[6][7] The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set 





C




{\displaystyle {\mathcal {C}}}

. The step that occurs at i-th iteration is described by this expression:






D


i


=


proj



C




{



D


i
−
1


−

δ

i



∇


D




∑

i
∈
S


‖

x

i


−

D


r

i



‖

2


2


+
λ
‖

r

i



‖

1



}



{\displaystyle \mathbf {D} _{i}={\text{proj}}_{\mathcal {C}}\left\{\mathbf {D} _{i-1}-\delta _{i}\nabla _{\mathbf {D} }\sum _{i\in S}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right\}}

, where 



S


{\displaystyle S}

 is a random subset of 



{
1...
K
}


{\displaystyle \{1...K\}}

 and 




δ

i




{\displaystyle \delta _{i}}

 is a gradient step.
An algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function.[8] Consider the following Lagrangian:






L


(

D

,
Λ
)
=

tr


(

(
X
−

D

R

)

T


(
X
−

D

R
)

)

+

∑

j
=
1


n



λ

i



(


∑

i
=
1


d




D


i
j


2


−
c

)



{\displaystyle {\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}\left((X-\mathbf {D} R)^{T}(X-\mathbf {D} R)\right)+\sum _{j=1}^{n}\lambda _{i}\left({\sum _{i=1}^{d}\mathbf {D} _{ij}^{2}-c}\right)}

, where 



c


{\displaystyle c}

 is a constraint on the norm of the atoms and 




λ

i




{\displaystyle \lambda _{i}}

 are the so-called dual variables forming the diagonal matrix 



Λ


{\displaystyle \Lambda }

.
We can then provide an analytical expression for the Lagrange dual after minimization over 




D



{\displaystyle \mathbf {D} }

:






D


(
Λ
)
=

min


D





L


(

D

,
Λ
)
=

tr

(

X

T


X
−
X

R

T


(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T


−
c
Λ
)


{\displaystyle {\mathcal {D}}(\Lambda )=\min _{\mathbf {D} }{\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}(X^{T}X-XR^{T}(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}-c\Lambda )}

.
After applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient) we get the value of 




D



{\displaystyle \mathbf {D} }

:






D


T


=
(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T




{\displaystyle \mathbf {D} ^{T}=(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}}


Solving this problem is less computational hard because the amount of dual variables 



n


{\displaystyle n}

 is a lot of times much less than the amount of variables in the primal problem.
In this approach, the optimization problem is formulated as:





min

r
∈


R


n




{


‖
r

‖

1


}



subject to



‖
X
−

D

R

‖

F


2


<
ϵ


{\displaystyle \min _{r\in \mathbb {R} ^{n}}\{\,\,\|r\|_{1}\}\,\,{\text{subject to}}\,\,\|X-\mathbf {D} R\|_{F}^{2}<\epsilon }

, where 



ϵ


{\displaystyle \epsilon }

is the permitted error in the reconstruction LASSO.
It finds an estimate of 




r

i




{\displaystyle r_{i}}

by minimizing the least square error subject to a L1-norm constraint in the solution vector, formulated as:





min

r
∈


R


n









1
2





‖
X
−

D

r

‖

F


2


+
λ


‖
r

‖

1




{\displaystyle \min _{r\in \mathbb {R} ^{n}}\,\,{\dfrac {1}{2}}\,\,\|X-\mathbf {D} r\|_{F}^{2}+\lambda \,\,\|r\|_{1}}

, where 



λ
>
0


{\displaystyle \lambda >0}

controls the trade-off between sparsity and the reconstruction error. This gives the global optimal solution[9]. See also Online dictionary learning for Sparse coding
Parametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones.[10] This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include: 
Many common approaches to sparse dictionary learning rely on the fact that the whole input data 



X


{\displaystyle X}

 (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points 



x


{\displaystyle x}

 becoming available.
A dictionary can be learned in an online manner the following way:[14]
This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).
The dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.
It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.[15]
Sparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis[16] and unsupervised clustering.[17] In evaluations with the Bag-of-Words model,[18][19] sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.
Dictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), functional MRI (fMRI), and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.
"
https://en.wikipedia.org/wiki/Anomaly_detection,"In data mining, anomaly detection (also outlier detection[1]) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[1] Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.[2]
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.[3]
Three broad categories of anomaly detection techniques exist.[4] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the learnt model.
Anomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting ecosystem disturbances. It is often used in preprocessing to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.[5][6]
Several anomaly detection techniques have been proposed in literature.[7] Some of the popular techniques are:
The performance of different methods depends a lot on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.[30][31]
Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986.[32] Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning.[33] Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.[34]  The counterpart of anomaly detection in intrusion detection is misuse detection.
"
https://en.wikipedia.org/wiki/Association_rule_learning,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1]
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami[2] introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Following the original definition by Agrawal, Imieliński, Swami[2] the problem of association rule mining is defined as:
Let 



I
=
{

i

1


,

i

2


,
…
,

i

n


}


{\displaystyle I=\{i_{1},i_{2},\ldots ,i_{n}\}}

 be a set of 



n


{\displaystyle n}

 binary attributes called items.
Let 



D
=
{

t

1


,

t

2


,
…
,

t

m


}


{\displaystyle D=\{t_{1},t_{2},\ldots ,t_{m}\}}

 be a set of transactions called the database.
Each transaction in 



D


{\displaystyle D}

 has a unique transaction ID and contains a subset of the items in 



I


{\displaystyle I}

.
A rule is defined as an implication of the form:




X
⇒
Y


{\displaystyle X\Rightarrow Y}

, where 



X
,
Y
⊆
I


{\displaystyle X,Y\subseteq I}

.
In Agrawal, Imieliński, Swami[2] a rule is defined only between a set and a single item, 



X
⇒

i

j




{\displaystyle X\Rightarrow i_{j}}

 for 




i

j


∈
I


{\displaystyle i_{j}\in I}

.
Every rule is composed by two different sets of items, also known as itemsets, 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

, where 



X


{\displaystyle X}

 is called antecedent or left-hand-side (LHS) and 



Y


{\displaystyle Y}

 consequent or right-hand-side (RHS).
To illustrate the concepts, we use a small example from the supermarket domain. The set of items is 



I
=
{

m
i
l
k
,
b
r
e
a
d
,
b
u
t
t
e
r
,
b
e
e
r
,
d
i
a
p
e
r
s

}


{\displaystyle I=\{\mathrm {milk,bread,butter,beer,diapers} \}}

 and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.
An example rule for the supermarket could be 



{

b
u
t
t
e
r
,
b
r
e
a
d

}
⇒
{

m
i
l
k

}


{\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}

 meaning that if butter and bread are bought, customers also buy milk.
Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant[citation needed], and datasets often contain thousands or millions of transactions.
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
Let 



X
,
Y


{\displaystyle X,Y}

 be itemsets, 



X
⇒
Y


{\displaystyle X\Rightarrow Y}

 an association rule and 



T


{\displaystyle T}

 a set of transactions of a given database.
Support is an indication of how frequently the itemset appears in the dataset.
The support of 



X


{\displaystyle X}

 with respect to 



T


{\displaystyle T}

 is defined as the proportion of transactions 



t


{\displaystyle t}

 in the dataset which contains the itemset 



X


{\displaystyle X}

.





s
u
p
p

(
X
)
=




|

{
t
∈
T
;
X
⊆
t
}

|




|

T

|






{\displaystyle \mathrm {supp} (X)={\frac {|\{t\in T;X\subseteq t\}|}{|T|}}}


In the example dataset, the itemset 



X
=
{

b
e
e
r
,
d
i
a
p
e
r
s

}


{\displaystyle X=\{\mathrm {beer,diapers} \}}

 has a support of 



1

/

5
=
0.2


{\displaystyle 1/5=0.2}

 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of 




s
u
p
p

(
)


{\displaystyle \mathrm {supp} ()}

 is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).[3]
Confidence is an indication of how often the rule has been found to be true.
The confidence value of a rule, 



X
⇒
Y


{\displaystyle X\Rightarrow Y}

 , with respect to a set of transactions 



T


{\displaystyle T}

, is the proportion of the transactions that contains 



X


{\displaystyle X}

 which also contains 



Y


{\displaystyle Y}

.
Confidence is defined as:





c
o
n
f

(
X
⇒
Y
)
=

s
u
p
p

(
X
∪
Y
)

/


s
u
p
p

(
X
)


{\displaystyle \mathrm {conf} (X\Rightarrow Y)=\mathrm {supp} (X\cup Y)/\mathrm {supp} (X)}


For example, the rule 



{

b
u
t
t
e
r
,
b
r
e
a
d

}
⇒
{

m
i
l
k

}


{\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}

 has a confidence of 



0.2

/

0.2
=
1.0


{\displaystyle 0.2/0.2=1.0}

 in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).
Note that 




s
u
p
p

(
X
∪
Y
)


{\displaystyle \mathrm {supp} (X\cup Y)}

 means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of events and not sets of items. We can rewrite 




s
u
p
p

(
X
∪
Y
)


{\displaystyle \mathrm {supp} (X\cup Y)}

 as the probability 



P
(

E

X


∩

E

Y


)


{\displaystyle P(E_{X}\cap E_{Y})}

, where 




E

X




{\displaystyle E_{X}}

 and 




E

Y




{\displaystyle E_{Y}}

 are the events that a transaction contains itemset 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

, respectively.[4]
Thus confidence can be interpreted as an estimate of the conditional probability 



P
(

E

Y



|


E

X


)


{\displaystyle P(E_{Y}|E_{X})}

, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.[3][5]
The lift of a rule is defined as:





l
i
f
t

(
X
⇒
Y
)
=




s
u
p
p

(
X
∪
Y
)



s
u
p
p

(
X
)
×

s
u
p
p

(
Y
)





{\displaystyle \mathrm {lift} (X\Rightarrow Y)={\frac {\mathrm {supp} (X\cup Y)}{\mathrm {supp} (X)\times \mathrm {supp} (Y)}}}


or the ratio of the observed support to that expected if X and Y were independent.
For example, the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 has a lift of 





0.2

0.4
×
0.4



=
1.25


{\displaystyle {\frac {0.2}{0.4\times 0.4}}=1.25}

.
If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.
If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.
If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.
The value of lift is that it considers both the support of the rule and the overall data set.[3]
The conviction of a rule is defined as 




c
o
n
v

(
X
⇒
Y
)
=



1
−

s
u
p
p

(
Y
)


1
−

c
o
n
f

(
X
⇒
Y
)





{\displaystyle \mathrm {conv} (X\Rightarrow Y)={\frac {1-\mathrm {supp} (Y)}{1-\mathrm {conf} (X\Rightarrow Y)}}}

.[6]
For example, the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 has a conviction of 






1
−
0.4


1
−
0.5



=
1.2


{\displaystyle {\frac {1-0.4}{1-0.5}}=1.2}

, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 would be incorrect 20% more often (1.25 times as often) if the association between X and Y was purely random chance.
In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:
Several more measures are presented and compared by Tan et al.[10] and by Hahsler.[4] Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of ""Subjective Interestingness.""

 Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
While the second step is straightforward, the first step needs more attention.
Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations). The set of possible itemsets is the power set over 



I


{\displaystyle I}

 and has size 




2

n


−
1


{\displaystyle 2^{n}-1}

 (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items 



n


{\displaystyle n}

 in 



I


{\displaystyle I}

, efficient search is possible using the downward-closure property of support[2][11] (also called anti-monotonicity[12]) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori[13] and Eclat[14]) can find all frequent itemsets.
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,[2] which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, what is now called ""association rules"" is introduced already in the 1966 paper[15] on GUHA, a general data mining method developed by Petr Hájek et al.[16]
An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with 




s
u
p
p

(
X
)


{\displaystyle \mathrm {supp} (X)}

 and 




c
o
n
f

(
X
⇒
Y
)


{\displaystyle \mathrm {conf} (X\Rightarrow Y)}

 greater than user defined constraints.[17]
One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery[18][19] controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.
Many algorithms for generating association rules have been proposed.
Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.
Apriori[13] uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.
Eclat[14] (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm based on set intersection. It is suitable for both sequential as well as parallel execution with locality-enhancing properties.[20][21]
FP stands for frequent pattern.[22]
In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.
Items in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.
Items in each transaction that do not meet the minimum support requirement are discarded.
If many transactions share most frequent items, the FP-tree provides high compression close to tree root.
Recursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).
Growth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item 



I


{\displaystyle I}

.
A new conditional tree is created which is the original FP-tree projected onto 



I


{\displaystyle I}

. The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on 



I


{\displaystyle I}

 meet the minimum support threshold. The resulting paths from root to 



I


{\displaystyle I}

 will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.
Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.[23]
The ASSOC procedure[24] is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example ""items"" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.
OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.[25] Initially used to find rules for a fixed consequent[25][26] it has subsequently been extended to find rules with any item as a consequent.[27] OPUS search is the core technology in the popular Magnum Opus association discovery system.
A famous story about association rule mining is the ""beer and diaper"" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.[28] Daniel Powers says:[28]
In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis ""did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers"". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) are association rules where each item may have several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 -> their health condition is good”. Such association rules are extractable from RDBMS data or semantic web data.[29]
Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.[30][31]
Weighted class learning is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.
High-order pattern discovery facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
[32]
K-optimal pattern discovery provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.
Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.[33]
Generalized Association Rules hierarchical taxonomy (concept hierarchy)
Quantitative Association Rules categorical and quantitative data
Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
Sequential pattern mining  discovers subsequences that are common to more than minsup[clarification needed] sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.[34]
Subspace Clustering, a specific type of Clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.[35]
Warmr is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.[36]
"
https://en.wikipedia.org/wiki/Inductive_logic_programming,"Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[1][2][3] Shapiro built their first implementation (Model Inference System) in 1981:[4] a Prolog program that inductively inferred logic programs from positive and negative examples. The term Inductive Logic Programming was first introduced[5] in a paper by Stephen Muggleton in 1991.[6] Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution,[7] and Inverse entailment.[8] Muggleton implemented Inverse entailment first in the PROGOL system. The term ""inductive"" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction.
The background knowledge is given as a logic theory B, commonly in the form of Horn clauses used in logic programming.
The positive and negative examples are given as a conjunction 




E

+




{\displaystyle E^{+}}

 and 




E

−




{\displaystyle E^{-}}

 of unnegated and negated ground literals, respectively.
A correct hypothesis h is a logic proposition satisfying the following requirements.[9]
""Necessity"" does not impose a restriction on h, but forbids any generation of a hypothesis as long as the positive facts are explainable without it.
""Sufficiency"" requires any generated hypothesis h to explain all positive examples 




E

+




{\displaystyle E^{+}}

.
""Weak consistency"" forbids generation of any hypothesis h that contradicts the background knowledge B.
""Strong consistency"" also forbids generation of any hypothesis h that is inconsistent with the negative examples 




E

−




{\displaystyle E^{-}}

, given the background knowledge B; it implies ""Weak consistency""; if no negative examples are given, both requirements coincide. Džeroski [10] requires only ""Sufficiency"" (called ""Completeness"" there) and ""Strong consistency"".
The following well-known example about learning definitions of family relations uses the abbreviations 
It starts from the background knowledge (cf. picture)
the positive examples
and the trivial proposition
true
to denote the absence of negative examples.
Plotkin's [11][12] ""relative least general generalization (rlgg)"" approach to inductive logic programming shall be used to obtain a suggestion about how to formally define the daughter relation dau.
This approach uses the following steps.
The resulting Horn clause is the hypothesis h obtained by the rlgg approach. Ignoring the background knowledge facts, the clause informally reads ""




x

m
e




{\displaystyle x_{me}}

 is called a daughter of 




x

h
t




{\displaystyle x_{ht}}

 if 




x

h
t




{\displaystyle x_{ht}}

 is the parent of 




x

m
e




{\displaystyle x_{me}}

 and 




x

m
e




{\displaystyle x_{me}}

 is female"", which is a commonly accepted definition.
Concerning the above requirements, ""Necessity"" was satisfied because the predicate dau doesn't appear in the background knowledge, which hence cannot imply any property containing this predicate, such as the positive examples are.
""Sufficiency"" is satisfied by the computed hypothesis h, since it, together with 





par


(
h
,
m
)
∧


fem


(
m
)


{\displaystyle {\textit {par}}(h,m)\land {\textit {fem}}(m)}

 from the background knowledge, implies the first positive example 





dau


(
m
,
h
)


{\displaystyle {\textit {dau}}(m,h)}

, and similarly  h and 





par


(
t
,
e
)
∧


fem


(
e
)


{\displaystyle {\textit {par}}(t,e)\land {\textit {fem}}(e)}

 from the background knowledge implies the second  positive example 





dau


(
e
,
t
)


{\displaystyle {\textit {dau}}(e,t)}

. ""Weak consistency"" is satisfied by h, since h holds in the (finite) Herbrand structure described by the background knowledge; similar for ""Strong consistency"".
The common definition of the grandmother relation, viz. 





gra


(
x
,
z
)
←


fem


(
x
)
∧


par


(
x
,
y
)
∧


par


(
y
,
z
)


{\displaystyle {\textit {gra}}(x,z)\leftarrow {\textit {fem}}(x)\land {\textit {par}}(x,y)\land {\textit {par}}(y,z)}

, cannot be learned using the above approach, since the variable y occurs in the clause body only; the corresponding literals would have been deleted in the 4th step of the approach. To overcome this flaw, that step has to be modified such that it can be parametrized with different literal post-selection heuristics. Historically, the GOLEM implementation is based on the rlgg approach.
Inductive Logic Programming system is a program that takes as an input logic theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}

 and outputs a correct hypothesis H wrt theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}

 An algorithm of an ILP system consists of two parts: hypothesis search and hypothesis selection. First a hypothesis is searched with an inductive logic programming procedure, then a subset of the found hypotheses (in most systems one hypothesis) is chosen by a selection algorithm. A selection algorithm scores each of the found hypotheses and returns the ones with the highest score. An example of score function include minimal compression length where a hypothesis with a lowest Kolmogorov complexity has the highest score and is returned. An ILP system is complete iff for any input logic theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}

 any correct hypothesis H wrt to these input theories can be found with its hypothesis search procedure.
Modern ILP systems like Progol,[6] Hail [15] and Imparo [16] find a hypothesis H using the principle of the inverse entailment[6] for theories B, E, H: 



B
∧
H
⊨
E

⟺

B
∧
¬
E
⊨
¬
H


{\displaystyle B\land H\models E\iff B\land \neg E\models \neg H}

. First they construct an intermediate theory F called a bridge theory satisfying the conditions 



B
∧
¬
E
⊨
F


{\displaystyle B\land \neg E\models F}

 and 



F
⊨
¬
H


{\displaystyle F\models \neg H}

. Then as 



H
⊨
¬
F


{\displaystyle H\models \neg F}

, they generalize the negation of the bridge theory F with the anti-entailment.[17] However, the operation of the anti-entailment since being highly non-deterministic is computationally more expensive. Therefore, an alternative hypothesis search can be conducted using the operation of the inverse subsumption (anti-subsumption) instead which is less non-deterministic than anti-entailment.
Questions of completeness of a hypothesis search procedure of specific ILP system arise. For example, Progol's hypothesis search procedure based on the inverse entailment inference rule is not complete by Yamamoto's example.[18] On the other hand, Imparo is complete by both anti-entailment procedure [19] and its extended inverse subsumption [20] procedure.
"
https://en.wikipedia.org/wiki/Artificial_neural_network,"Topics
Collective intelligence
Collective action
Self-organized criticality
Herd mentality
Phase transition
Agent-based modelling
Synchronization
Ant colony optimization
Particle swarm optimization
Swarm behaviour
Social network analysis
Small-world networks
Community identification
Centrality
Motifs
Graph Theory
Scaling
Robustness
Systems biology
Dynamic networks
Evolutionary computation
Genetic algorithms
Genetic programming
Artificial life
Machine learning
Evolutionary developmental biology
Artificial intelligence
Evolutionary robotics
Reaction–diffusion systems
Partial differential equations
Dissipative structures
Percolation
Cellular automata
Spatial ecology
Self-replication
Spatial evolutionary biology
Operationalization
Feedback
Self-reference
Goal-oriented
System dynamics
Sensemaking
Entropy
Cybernetics
Autopoiesis
Information theory
Computation theory
Ordinary differential equations
Iterative maps
Phase space
Attractors
Stability analysis
Population dynamics
Chaos
Multistability
Bifurcation
Rational choice theory
Bounded rationality
Irrational behaviour

Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains.[1] Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.
In ANN implementations, the ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. But over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games, medical diagnosis, and even in activities that have traditionally been considered as reserved to humans, like painting.[2]
Warren McCulloch and Walter Pitts[3] (1943) opened the subject by creating a computational model for neural networks.[4] In the late 1940s, D. O. Hebb[5] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark[6] (1954) first used computational machines, then called ""calculators"", to simulate a Hebbian network. Rosenblatt[7] (1958) created the perceptron.[8] The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling.[9][10][11] The basics of continuous backpropagation[9][12][13][14]  were derived in the context of control theory by Kelley[15] in 1960 and by Bryson in 1961,[16] using principles of dynamic programming.
In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.[17][18] In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.[19] Werbos's (1975) backpropagation algorithm enabled practical training of multi-layer networks. In 1982, he applied Linnainmaa's AD method to neural networks in the way that became widely used.[12][20] Thereafter research stagnated following Minsky and Papert (1969),[21] who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition.[22][23][24] Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.[25]
The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural networks in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.[26]
Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[27] to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.[28] Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as ""deep learning""[29].
Ciresan and colleagues (2010)[30] showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks.[31] Between 2009 and 2012, ANNs began winning prizes in ANN contests, approaching human level performance on various tasks, initially in pattern recognition and machine learning.[32][33] For example, the bi-directional and multi-dimensional long short-term memory (LSTM)[34][35][36][37] of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.[36][35]
Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance[38] on benchmarks such as traffic sign recognition (IJCNN 2012).
ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors. Neurons are connected to each other in various patterns, to allow the output of some neurons to become the input of others. The network forms a directed, weighted graph.[39]
An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections. Each link has a weight, which determines the strength of one node's influence on another.[40]
ANNs are composed of artificial neurons which retain the biological concept of neurons, which receive input, combine the input with their internal state (activation) and an optional threshold using an activation function, and produce output using an output function. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image. The important characteristic of the activation function is that it provides a smooth, differentiable transition as input values change, i.e. a small change in input produces a small change in output.[41]
The network consists of connections, each connection providing the output of one neuron as an input to another neuron. Each connection is assigned a weight that represents its relative importance.[39] A given neuron can have multiple input and output connections.[42]
The propagation function computes the input to a neuron from the outputs of its predecessor neurons and their connections as a weighted sum.[39] A bias term can be added to the result of the propagation.[43]
The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be fully connected, with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connect to a single neuron in the next layer, thereby reducing the number of neurons in that layer.[44] Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.[45] Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.[46]
A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.[47] The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.
Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations.[39] Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.
The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.[48] The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.
While it is possible to define a cost function  ad hoc, frequently the choice is determined by the functions desirable properties (such as convexity) or because it arises from the model (e.g., in a probabilistic model the model's posterior probability can be used as an inverse cost).
Backpropagation is a method to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as Extreme Learning Machines,[49] ""No-prop"" networks,[50] training without backtracking,[51] ""weightless"" networks,[52][53] and non-connectionist neural networks.
The three major learning paradigms are supervised learning, unsupervised learning and reinforcement learning. They each correspond to a particular learning task
Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case the cost function is related to eliminating incorrect deductions.[54] A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a ""teacher"", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
In unsupervised learning, input data is given along with the cost function, some function of the data 




x



{\displaystyle \textstyle x}

 and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model 




f
(
x
)
=
a



{\displaystyle \textstyle f(x)=a}

 where 




a



{\displaystyle \textstyle a}

 is a constant and the cost 




C
=
E
[
(
x
−
f
(
x
)

)

2


]



{\displaystyle \textstyle C=E[(x-f(x))^{2}]}

. Minimizing this cost produces a value of 




a



{\displaystyle \textstyle a}

 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between 




x



{\displaystyle \textstyle x}

 and 




f
(
x
)



{\displaystyle \textstyle f(x)}

, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.
Formally the environment is modeled as a Markov decision process (MDP) with states 






s

1


,
.
.
.
,

s

n



∈
S



{\displaystyle \textstyle {s_{1},...,s_{n}}\in S}

 and actions 






a

1


,
.
.
.
,

a

m



∈
A



{\displaystyle \textstyle {a_{1},...,a_{m}}\in A}

. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution 




P
(

c

t



|


s

t


)



{\displaystyle \textstyle P(c_{t}|s_{t})}

, the observation distribution 




P
(

x

t



|


s

t


)



{\displaystyle \textstyle P(x_{t}|s_{t})}

 and the transition distribution 




P
(

s

t
+
1



|


s

t


,

a

t


)



{\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})}

, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.
ANNs serve as the learning component in such applications.[55][56] Dynamic programming coupled with ANNs (giving neurodynamic programming)[57] has been applied to problems such as those involved in vehicle routing,[58] video games, natural resource management[59][60] and medicine[61] because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
Self learning in neural networks was introduced in 1982 along with a neural network capable of self-learning  named Crossbar Adaptive Array (CAA).[62] It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion.[63] Given memory matrix W =||w(a,s)||, the crossbar self learning algorithm in each iteration performs the following computation:
The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.[64]
In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods,[65] gene expression programming,[66] simulated annealing,[67] expectation-maximization, non-parametric methods and particle swarm optimization[68] are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.[69][70]
Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces ""noise"" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use ""mini-batches"", small batches with samples in each batch selected stochastically from the entire data set.
ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be ""supervised"" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
Some of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data;[71][72] long short-term memory avoid the vanishing gradient problem[73] and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition,[74][75] text-to-speech synthesis,[76][12][77] and photo-real talking heads;[78] competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game[79] or on deceiving the opponent about the authenticity of an input.[80]
Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network.[81] Available systems include AutoML and AutoKeras.[82]
Design issues include deciding the number, type and connectedness of network layers, as well as the size of each and the connection type (full, pooling, ...).
Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.[83]
Using Artificial neural networks requires an understanding of their characteristics.
ANN capabilities fall within the following broad categories:[citation needed]
Because of their ability to reproduce and model nonlinear processes, Artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction,[84] process control, natural resource management), quantum chemistry,[85] general game playing,[86] pattern recognition (radar systems, face identification, signal classification,[87] 3D reconstruction,[88] object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance[89] (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering[90] and e-mail spam filtering. ANNs have been used to diagnose cancers, including lung cancer,[91] prostate cancer, colorectal cancer[92] and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.[93][94]
ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters[95][96] and to predict foundation settlements.[97] ANNs have also been used for building black-box models in geoscience: hydrology,[98][99] ocean modelling and coastal engineering,[100][101] and geomorphology.[102] ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware,[103] for identifying domains belonging to threat actors and for detecting URLs posing a security risk.[104] Research is underway on ANN systems designed for penetration testing, for detecting botnets,[105] credit cards frauds[106] and network intrusions.
ANNs have been proposed as a tool to simulate the properties of many-body open quantum systems.[107][108][109][110] In brain research ANNs have studied short-term behavior of individual neurons,[111] the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.
A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine,[112] using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.[113]
A model's ""capacity"" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book [114] which summarizes work by Thomas Cover [115]. The capacity of a network of standard neurons (not convolutional) can be derived by four rules [116] that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of  measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in [117], the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity [118].
Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.
Convergence behavior of certain types of ANN architectures are more understood than others. Such as when the width of network approaches to infinity, the ANN resembles linear model, thus such ANN follows the convergence behavior of linear model also.[119] Another example is when parameters are small, it is observed that  ANN often fits target functions from low to high frequencies.[120][121][122][123] Such phenomenon is in opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.
Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.
The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:

A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation.[citation needed] Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.[69]
A fundamental objection is that ANNs do not sufficiently reflect neuronal function. Backpropagation is a critical step, although no such mechanism exists in biological neural networks.[124] How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently.[125] Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.
A central claim of ANNs is that they embody new and powerful general principles for processing information. Unfortunately, these principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a ""something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything"".[126] One response to Dewdney is that neural networks handle many complex and diverse tasks, ranging from autonomously flying aircraft[127] to detecting credit card fraud to mastering the game of Go.
Technology writer Roger Bridgman commented:
Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[128]
Biological brains use both shallow and deep circuits as reported by brain anatomy,[129] displaying a wide variety of invariance. Weng[130] argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.
Large and effective neural networks require considerable computing resources.[131] While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.
Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.[9] The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.[132][131]
Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.[133]
Analyzing what has been learned by an ANN, is much easier than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.[134]
Advocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.[135][136]
A single-layer feedforward artificial neural network. Arrows originating from 





x

2





{\displaystyle \scriptstyle x_{2}}

 are omitted for clarity. There are p inputs to this network and q outputs. In this system, the value of the qth output, 





y

q





{\displaystyle \scriptstyle y_{q}}

 would be calculated as 





y

q


=
K
∗
(
∑
(

x

i


∗

w

i
q


)
−

b

q


)



{\displaystyle \scriptstyle y_{q}=K*(\sum (x_{i}*w_{iq})-b_{q})}


A two-layer feedforward artificial neural network.
An artificial neural network.
An ANN dependency graph.
A single-layer feedforward artificial neural network with 4 inputs, 6 hidden and 2 outputs. Given position state and direction outputs wheel based control values.
A two-layer feedforward artificial neural network with 8 inputs, 2x8 hidden and 2 outputs. Given position state, direction and other environment values outputs thruster based control values.
Parallel pipeline structure of CMAC neural network. This learning algorithm can converge in one step.
"
https://en.wikipedia.org/wiki/Deep_learning,"Deep learning  (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]
Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[4][5][6]
Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9]
Deep learning is a class of machine learning algorithms that[11](pp199–200) uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.
Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[12]
In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1][13]
The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[14] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method.[15] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]
For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors[16] and deep belief networks.[1][17]
Deep neural networks are generally interpreted in terms of the universal approximation theorem[18][19][20][21][22][23] or probabilistic inference.[11][12][1][2][17][24][25]
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[18][19][20][21][22] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[19] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[20] Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.[26]
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[23] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[11][12][1][2][17][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[27] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[28]
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[29][16] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[30][31]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[32] A 1971 paper described already a deep network with 8 layers trained by the group method of data handling algorithm.[33]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[34] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[35][36][37][38] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[39]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[40][41][42] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[43]
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[44] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[45][46]
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[47][48][49] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[50] Key difficulties have been analyzed, including gradient diminishing[45] and weak temporal correlation structure in neural predictive models.[51][52] Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[53] The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[54]
The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s,[54] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[55]
Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[56] LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[57] Later it was combined with connectionist temporal classification (CTC)[58] in stacks of LSTM RNNs.[59] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[60]
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[61]
[62][63] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[64] The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[65][66][67] Convolutional neural networks (CNNs) were superseded for ASR by CTC[58] for LSTM.[56][60][68][69][70][71][72] but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[73] Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition[74] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[75] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[65][76] The nature of the recognition errors produced by the two types of systems was characteristically different,[77][74] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[11][78][79] Analysis around 2009–2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[77][74] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[65][77][75][80]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[81][82][83][78]
Advances in hardware have enabled renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[84] That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[85] In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.[86][87][88] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[89][90] Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.[91]
In 2012, a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug.[92][93] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the ""Tox21 Data Challenge"" of NIH, FDA and NCATS.[94][95][96]
Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.[86][88][39][97][2] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[98] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[99] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.[100]
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[101][102][103][104]
Some researchers assess that the October 2012 ImageNet victory anchored the start of a ""deep learning revolution"" that has transformed the AI industry.[105]
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing ""Go""[106] ).
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[12][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name ""deep"" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[107] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[12]
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[108] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[109][110][111][112][113] Long short-term memory is particularly effective for this use.[56][114]
Convolutional deep neural networks (CNNs) are used in computer vision.[115] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[72]
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[33] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[116] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[117] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[118]
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[119] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[120][121]
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[122][123]
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[114] is competitive with traditional speech recognizers on certain tasks.[57]
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[124] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[11][80][78]
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[11][129][130][131]
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[132]
Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011.[133]
Deep learning-trained vehicles now interpret 360° camera views.[134] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[135][136]
Neural networks have been used for implementing language models since the early 2000s.[109][137] LSTM helped to improve machine translation and language modeling.[110][111][112]
Other key techniques in this field are negative sampling[138] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[139] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[139] Deep neural architectures provide the best results for constituency parsing,[140] sentiment analysis,[141] information retrieval,[142][143] spoken language understanding,[144] machine translation,[110][145] contextual entity linking,[145] writing style recognition,[146] Text classification and others.[147]
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory network.[148][149][150][151][152][153] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples.""[149] It translates ""whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[149] The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"".[149][154] GT uses English as an intermediate between most language pairs.[154]
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[155][156] Research has explored use of deep learning to predict the biomolecular targets,[92][93] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[94][95][96]
AtomNet is a deep learning system for structure-based rational drug design.[157] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[158] and multiple sclerosis.[159][160]
In 2019 generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[161][162]
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[163]
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[164][165] Multiview deep learning has been applied for learning user preferences from multiple domains.[166] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[167]
In medical informatics, deep learning was used to predict sleep quality based on data from wearables[168] and predictions of health complications from electronic health record data.[169] Deep learning has also showed efficacy in healthcare.[170]
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[171][172]
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[173] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[174] These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration""[175] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
Deep learning is being successfully applied to financial fraud detection and anti-money laundering. ""Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events"". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.
[176]
The United States Department of Defense applied deep learning to train robots in new tasks through observation.[177]
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[178][179][180][181] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.""[182]
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[183][184] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[185][186] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[187]
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[188][189] and neural populations.[190] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[191] both at the single-unit[192] and at the population[193] levels.
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[194]
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[195][196][197] Google Translate uses a neural network to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.[198]
In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[199]
As of 2008,[200] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[177] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[177] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[201]
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
A main criticism concerns the lack of theory surrounding some methods.[202] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[203]

Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:""Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.""[204]As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between ""old master"" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy.[205] This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.[206]
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[207] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[208] website.
Some deep learning architectures display problematic behaviors,[209] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images[210] and misclassifying minuscule perturbations of correctly classified images.[211] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[209] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[212] decompositions of observed entities and events.[209] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[213] and artificial intelligence (AI).[214]
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception.[215] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.”[216] In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[217] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[218]
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[217]
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[217]
Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.[217]
In “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[217]
Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[219] The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[219] Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture.[220] This user interface is a mechanism to generate ""a constant stream of  verification data""[219] to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as ""human-aided artificial intelligence"".[219]
Shallowing refers to reducing a pre-trained DNN to a smaller network with the same or similar performance.[221] Training of DNN with further shallowing can produce more efficient systems than just training of smaller networks from scratch. Shallowing is the rebirth of pruning that developed in the 1980-1990s.[222][223] The main approach to pruning is to gradually remove network elements (synapses, neurons, blocks of neurons, or layers) that have little impact on performance evaluation. Various indicators of sensitivity are used that estimate the changes of performance after pruning. The simplest indicators use just values of transmitted signals and the synaptic weights (the zeroth order). More complex indicators use mean absolute values of partial derivatives of the cost function,[223][224] 
or even the second derivatives.[222] The shallowing allows to reduce the necessary resources and makes the skills of neural network more explicit.[224] It is used for image classification,[225] for development of security systems,[226] for accelerating DNN execution on mobile devices,[227] and for other applications. It has been demonstrated that using linear postprocessing, such as supervised PCA, improves DNN performance after shallowing.[226]
"
https://en.wikipedia.org/wiki/Decision_tree_learning,"Decision tree learning is one of the predictive modeling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.
In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making). This page deals with decision trees in data mining.
Decision tree learning is a method commonly used in data mining.[1] The goal is to create a model that predicts the value of a target variable based on several input variables. 
A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the ""classification"". Each element of the domain of the classification is called a class.
A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target or output feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes).
A tree is built by splitting the source set, constituting the root node of the tree, into subsets - which constitute the successor children. The splitting is based on a set of splitting rules based on classification features.[2]  This process is repeated on each derived subset in a recursive manner called recursive partitioning.
The recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT)[3] is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data[citation needed].
In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.
Data comes in records of the form:
The dependent variable, 



Y


{\displaystyle Y}

, is the target variable that we are trying to understand, classify or generalize. The vector 





x




{\displaystyle {\textbf {x}}}

 is composed of the features, 




x

1


,

x

2


,

x

3




{\displaystyle x_{1},x_{2},x_{3}}

 etc., that are used for that task.
Decision trees used in data mining are of two main types:
The term Classification And Regression Tree (CART) analysis is an umbrella term used to refer to both of the above procedures, first introduced by Breiman et al. in 1984.[4] Trees used for regression and trees used for classification have some similarities - but also some differences, such as the procedure used to determine where to split.[4]
Some techniques, often called ensemble methods, construct more than one decision tree:
A special case of a decision tree is a decision list,[9] which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node).  While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods[10] and monotonic constraints to be imposed.[11]
Decision tree learning is the construction of a decision tree from class-labeled training tuples. A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node.
There are many specific decision-tree algorithms. Notable ones include:
ID3 and CART were invented independently at around the same time (between 1970 and 1980)[citation needed], yet follow a similar approach for learning a decision tree from training tuples.
Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.[15] Different algorithms use different metrics for measuring ""best"".  These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split.
Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability 




p

i




{\displaystyle p_{i}}

 of an item with label 



i


{\displaystyle i}

  being chosen times the probability 




∑

k
≠
i



p

k


=
1
−

p

i




{\displaystyle \sum _{k\neq i}p_{k}=1-p_{i}}

 of a mistake in categorizing that item.  It reaches its minimum (zero) when all cases in the node fall into a single target category. 
The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient 



q
=
2


{\displaystyle q=2}

, which in Physics is associated with the lack of information in out-of-equlibrium, non-extensive, dissipative and quantum systems. For the limit 



q
→
1


{\displaystyle q\to 1}

 one recovers the usual Boltzmann-Gibbs or Shannon entropy. In this sense, the Gini impurity is but a variation of the usual entropy measure for decision trees.
To compute Gini impurity for a set of items with 



J


{\displaystyle J}

 classes, suppose 



i
∈
{
1
,
2
,
.
.
.
,
J
}


{\displaystyle i\in \{1,2,...,J\}}

, and let 




p

i




{\displaystyle p_{i}}

 be the fraction of items labeled with class 



i


{\displaystyle i}

 in the set.
Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.
Entropy is defined as below
where 




p

1


,

p

2


,
.
.
.


{\displaystyle p_{1},p_{2},...}

are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.[16]
Averaging over the possible values of 



A


{\displaystyle A}

,
That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information.
Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the purest daughter nodes. A commonly used measure of purity is called information which is measured in bits. For each node of the tree, the information value ""represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node"".[16]
Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or until the information gain is 0.
The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above. Since there is an equal number of yes's and no's in this node, we have
For the node where windy=false there were eight data points, six yes's and two no's. Thus we have
To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node.
To find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's.
Now we can calculate the information gain achieved by splitting on the windy feature.
To build the tree, the information gain of each possible first split would need to be calculated. The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al.[16]
Introduced in CART,[4] variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable x due to the split at this node:
where 



S


{\displaystyle S}

, 




S

t




{\displaystyle S_{t}}

, and 




S

f




{\displaystyle S_{f}}

 are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean.
Amongst other data mining methods, decision trees have various advantages:
Many data mining software packages provide implementations of one or more decision tree algorithms.
Examples include Salford Systems CART (which licensed the proprietary code of the original CART authors),[4] IBM SPSS Modeler, RapidMiner, SAS Enterprise Miner, Matlab, R (an open-source software environment for statistical computing, which includes several CART implementations such as rpart, party and randomForest packages), Weka (a free and open-source data-mining suite, contains many decision tree algorithms), Orange, KNIME, Microsoft SQL Server [1], and scikit-learn (a free and open-source machine learning library for the Python programming language).
In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML).[26]  Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.[27]  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.[citation needed]  In general, decision graphs infer models with fewer leaves than decision trees.
Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.[28][29]
It is also possible for a tree to be sampled using MCMC.[30]
The tree can be searched for in a bottom-up fashion.[31]
"
https://en.wikipedia.org/wiki/Support_vector_machines,"In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering[2] algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications.[citation needed]
Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a 



p


{\displaystyle p}

-dimensional vector (a list of 



p


{\displaystyle p}

 numbers), and we want to know whether we can separate such points with a 



(
p
−
1
)


{\displaystyle (p-1)}

-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.[citation needed]
More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection.[3] Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.[4]
Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed[by whom?] that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 



k
(
x
,
y
)


{\displaystyle k(x,y)}

 selected to suit the problem.[5] The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 




α

i




{\displaystyle \alpha _{i}}

 of images of feature vectors 




x

i




{\displaystyle x_{i}}

 that occur in the data base.[clarification needed] With this choice of a hyperplane, the points 



x


{\displaystyle x}

 in the feature space that are mapped into the hyperplane are defined by the relation 





∑

i



α

i


k
(

x

i


,
x
)
=

constant

.



{\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}

  Note that if 



k
(
x
,
y
)


{\displaystyle k(x,y)}

 becomes small as 



y


{\displaystyle y}

 grows further away from 



x


{\displaystyle x}

, each term in the sum measures the degree of closeness of the test point 



x


{\displaystyle x}

 to the corresponding data base point 




x

i




{\displaystyle x_{i}}

. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 



x


{\displaystyle x}

 mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.
SVMs can be used to solve various real-world problems:
The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.[15] The current standard[according to whom?] incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.[1]
We are given a training dataset of 



n


{\displaystyle n}

 points of the form
where the 




y

i




{\displaystyle y_{i}}

 are either 1 or −1, each indicating the class to which the point 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 belongs. Each 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 is a 



p


{\displaystyle p}

-dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 for which 




y

i


=
1


{\displaystyle y_{i}=1}

 from the group of points for which 




y

i


=
−
1


{\displaystyle y_{i}=-1}

, which is defined so that the distance between the hyperplane and the nearest point 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 from either group is maximized.
Any hyperplane can be written as the set of points 






x
→





{\displaystyle {\vec {x}}}

 satisfying
where 






w
→





{\displaystyle {\vec {w}}}

 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 






w
→





{\displaystyle {\vec {w}}}

 is not necessarily a unit vector. The parameter 






b

‖



w
→



‖






{\displaystyle {\tfrac {b}{\|{\vec {w}}\|}}}

 determines the offset of the hyperplane from the origin along the normal vector 






w
→





{\displaystyle {\vec {w}}}

.
If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the ""margin"", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations
and
Geometrically, the distance between these two hyperplanes is 






2

‖



w
→



‖






{\displaystyle {\tfrac {2}{\|{\vec {w}}\|}}}

,[16] so to maximize the distance between the planes we want to minimize 



‖



w
→



‖


{\displaystyle \|{\vec {w}}\|}

. The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each 



i


{\displaystyle i}

 either
or
These constraints state that each data point must lie on the correct side of the margin.
This can be rewritten as
We can put this together to get the optimization problem:
The 






w
→





{\displaystyle {\vec {w}}}

 and 



b


{\displaystyle b}

 that solve this problem determine our classifier, 






x
→



↦
sgn
⁡
(



w
→



⋅



x
→



−
b
)


{\displaystyle {\vec {x}}\mapsto \operatorname {sgn}({\vec {w}}\cdot {\vec {x}}-b)}

.
An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 that lie nearest to it. These 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 are called support vectors.
To extend SVM to cases in which the data are not linearly separable, we introduce the hinge loss function,
Note that 




y

i




{\displaystyle y_{i}}

 is the i-th target (i.e., in this case, 1 or −1), and 






w
→



⋅




x
→




i


−
b


{\displaystyle {\vec {w}}\cdot {\vec {x}}_{i}-b}

 is the current output.
This function is zero if the constraint in (1) is satisfied, in other words, if 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.
We then wish to minimize
where the parameter 



λ


{\displaystyle \lambda }

 determines the trade-off between increasing the margin size and ensuring that the 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 lie on the correct side of the margin. Thus, for sufficiently small values of 



λ


{\displaystyle \lambda }

, the second term in the loss function will become negligible, hence, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.
The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.[17]) to maximum-margin hyperplanes.[15] The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well.[18]
Some common kernels include:
The kernel is related to the transform 



φ
(




x

i


→



)


{\displaystyle \varphi ({\vec {x_{i}}})}

 by the equation 



k
(




x

i


→



,




x

j


→



)
=
φ
(




x

i


→



)
⋅
φ
(




x

j


→



)


{\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}

. The value w is also in the transformed space, with 







w
→



=

∑

i



α

i



y

i


φ
(




x
→




i


)



{\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}

. Dot products with w for classification can again be computed by the kernel trick, i.e. 







w
→



⋅
φ
(



x
→



)
=

∑

i



α

i



y

i


k
(




x
→




i


,



x
→



)



{\displaystyle \textstyle {\vec {w}}\cdot \varphi ({\vec {x}})=\sum _{i}\alpha _{i}y_{i}k({\vec {x}}_{i},{\vec {x}})}

.
Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form
We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }

 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.
Minimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.
For each 



i
∈
{
1
,

…
,

n
}


{\displaystyle i\in \{1,\,\ldots ,\,n\}}

 we introduce a variable 




ζ

i


=
max

(

0
,
1
−

y

i


(
w
⋅

x

i


−
b
)

)



{\displaystyle \zeta _{i}=\max \left(0,1-y_{i}(w\cdot x_{i}-b)\right)}

. Note that 




ζ

i




{\displaystyle \zeta _{i}}

 is the smallest nonnegative number satisfying 




y

i


(
w
⋅

x

i


−
b
)
≥
1
−

ζ

i


.


{\displaystyle y_{i}(w\cdot x_{i}-b)\geq 1-\zeta _{i}.}


Thus we can rewrite the optimization problem as follows
This is called the primal problem.
By solving for the Lagrangian dual of the above problem, one obtains the simplified problem
This is called the dual problem. Since the dual maximization problem is a quadratic function of the 




c

i




{\displaystyle c_{i}}

 subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.
Here, the variables 




c

i




{\displaystyle c_{i}}

 are defined such that
Moreover, 




c

i


=
0


{\displaystyle c_{i}=0}

 exactly when 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 lies on the correct side of the margin, and 



0
<

c

i


<
(
2
n
λ

)

−
1




{\displaystyle 0<c_{i}<(2n\lambda )^{-1}}

  when 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 lies on the margin's boundary. It follows that 






w
→





{\displaystyle {\vec {w}}}

 can be written as a linear combination of the support vectors.
The offset, 



b


{\displaystyle b}

, can be recovered by finding an 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 on the margin's boundary and solving
(Note that 




y

i


−
1


=

y

i




{\displaystyle y_{i}^{-1}=y_{i}}

 since 




y

i


=
±
1


{\displaystyle y_{i}=\pm 1}

.)
Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points 



φ
(




x
→




i


)
.


{\displaystyle \varphi ({\vec {x}}_{i}).}

 Moreover, we are given a kernel function 



k


{\displaystyle k}

 which satisfies 



k
(




x
→




i


,




x
→




j


)
=
φ
(




x
→




i


)
⋅
φ
(




x
→




j


)


{\displaystyle k({\vec {x}}_{i},{\vec {x}}_{j})=\varphi ({\vec {x}}_{i})\cdot \varphi ({\vec {x}}_{j})}

.
We know the classification vector 






w
→





{\displaystyle {\vec {w}}}

 in the transformed space satisfies
where, the 




c

i




{\displaystyle c_{i}}

 are obtained by solving the optimization problem
The coefficients 




c

i




{\displaystyle c_{i}}

 can be solved for using quadratic programming, as before. Again, we can find some index 



i


{\displaystyle i}

 such that 



0
<

c

i


<
(
2
n
λ

)

−
1




{\displaystyle 0<c_{i}<(2n\lambda )^{-1}}

, so that 



φ
(




x
→




i


)


{\displaystyle \varphi ({\vec {x}}_{i})}

 lies on the boundary of the margin in the transformed space, and then solve
Finally,
Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.
Sub-gradient descent algorithms for the SVM work directly with the expression
Note that 



f


{\displaystyle f}

 is a convex function of 






w
→





{\displaystyle {\vec {w}}}

 and 



b


{\displaystyle b}

. As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with 



n


{\displaystyle n}

, the number of data points.[19]
Coordinate descent algorithms for the SVM work from the dual problem
For each 



i
∈
{
1
,

…
,

n
}


{\displaystyle i\in \{1,\,\ldots ,\,n\}}

, iteratively, the coefficient 




c

i




{\displaystyle c_{i}}

 is adjusted in the direction of 



∂
f

/

∂

c

i




{\displaystyle \partial f/\partial c_{i}}

. Then, the resulting vector of coefficients 



(

c

1

′

,

…
,


c

n

′

)


{\displaystyle (c_{1}',\,\ldots ,\,c_{n}')}

 is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.[20]
The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.
In supervised learning, one is given a set of training examples 




X

1


…

X

n




{\displaystyle X_{1}\ldots X_{n}}

 with labels 




y

1


…

y

n




{\displaystyle y_{1}\ldots y_{n}}

, and wishes to predict 




y

n
+
1




{\displaystyle y_{n+1}}

 given 




X

n
+
1




{\displaystyle X_{n+1}}

. To do so one forms a hypothesis, 



f


{\displaystyle f}

, such that 



f
(

X

n
+
1


)


{\displaystyle f(X_{n+1})}

 is a ""good"" approximation of 




y

n
+
1




{\displaystyle y_{n+1}}

. A ""good"" approximation is usually defined with the help of a loss function, 



ℓ
(
y
,
z
)


{\displaystyle \ell (y,z)}

, which characterizes how bad 



z


{\displaystyle z}

 is as a prediction of 



y


{\displaystyle y}

. We would then like to choose a hypothesis that minimizes the expected risk:
In most cases, we don't know the joint distribution of 




X

n
+
1


,


y

n
+
1




{\displaystyle X_{n+1},\,y_{n+1}}

 outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:
Under certain assumptions about the sequence of random variables 




X

k


,


y

k




{\displaystyle X_{k},\,y_{k}}

 (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as 



n


{\displaystyle n}

 grows large. This approach is called empirical risk minimization, or ERM.
In order for the minimization problem to have a well-defined solution, we have to place constraints on the set 





H




{\displaystyle {\mathcal {H}}}

 of hypotheses being considered. If 





H




{\displaystyle {\mathcal {H}}}

 is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses 



f


{\displaystyle f}

 for which 



‖
f

‖


H



<
k


{\displaystyle \lVert f\rVert _{\mathcal {H}}<k}

 . This is equivalent to imposing a regularization penalty 





R


(
f
)
=

λ

k


‖
f

‖


H





{\displaystyle {\mathcal {R}}(f)=\lambda _{k}\lVert f\rVert _{\mathcal {H}}}

, and solving the new optimization problem
This approach is called Tikhonov regularization.
More generally, 





R


(
f
)


{\displaystyle {\mathcal {R}}(f)}

 can be some measure of the complexity of the hypothesis 



f


{\displaystyle f}

, so that simpler hypotheses are preferred.
Recall that the (soft-margin) SVM classifier 






w
^



,
b
:
x
↦
sgn
⁡
(



w
^



⋅
x
−
b
)


{\displaystyle {\hat {w}},b:x\mapsto \operatorname {sgn}({\hat {w}}\cdot x-b)}

 is chosen to minimize the following expression:
In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss
From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  




ℓ

s
q


(
y
,
z
)
=
(
y
−
z

)

2




{\displaystyle \ell _{sq}(y,z)=(y-z)^{2}}

; logistic regression employs the log-loss,
The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables 



X
,

y


{\displaystyle X,\,y}

.
In particular, let 




y

x




{\displaystyle y_{x}}

 denote 



y


{\displaystyle y}

 conditional on the event that 



X
=
x


{\displaystyle X=x}

.  In the classification setting, we have:
The optimal classifier is therefore:
For the square-loss, the target function is the conditional expectation function, 




f

s
q


(
x
)
=

E


[

y

x


]



{\displaystyle f_{sq}(x)=\mathbb {E} \left[y_{x}\right]}

; For the logistic loss, it's the logit function, 




f

log


(
x
)
=
ln
⁡

(


p

x



/

(

1
−

p

x



)

)



{\displaystyle f_{\log }(x)=\ln \left(p_{x}/({1-p_{x}})\right)}

. While both of these target functions yield the correct classifier, as 



sgn
⁡
(

f

s
q


)
=
sgn
⁡
(

f

log


)
=

f

∗




{\displaystyle \operatorname {sgn}(f_{sq})=\operatorname {sgn}(f_{\log })=f^{*}}

, they give us more information than we need. In fact, they give us enough information to completely describe the distribution of 




y

x




{\displaystyle y_{x}}

.
On the other hand, one can check that the target function for the hinge loss is exactly 




f

∗




{\displaystyle f^{*}}

. Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of 





R




{\displaystyle {\mathcal {R}}}

) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.[21]
SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.[22]
The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter C.
A common choice is a Gaussian kernel, which has a single parameter 



γ


{\displaystyle \gamma }

. The best combination of C and 



γ


{\displaystyle \gamma }

 is often selected by a grid search with exponentially growing sequences of C and 



γ


{\displaystyle \gamma }

, for example, 



C
∈
{

2

−
5


,

2

−
3


,
…
,

2

13


,

2

15


}


{\displaystyle C\in \{2^{-5},2^{-3},\dots ,2^{13},2^{15}\}}

; 



γ
∈
{

2

−
15


,

2

−
13


,
…
,

2

1


,

2

3


}


{\displaystyle \gamma \in \{2^{-15},2^{-13},\dots ,2^{1},2^{3}\}}

. Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select C and 



γ


{\displaystyle \gamma }

 , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.[23]
Potential drawbacks of the SVM include the following aspects:
SVC is a similar method that also builds on kernel functions but is appropriate for unsupervised learning. It is considered a fundamental method in data science.[citation needed]
Multiclass SVM aims to assign labels to instances by using support-vector machines, where the labels are drawn from a finite set of several elements.
The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems.[24] Common methods for such reduction include:[24][25]
Crammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems.[28] See also Lee, Lin and Wahba[29][30] and Van den Burg and Groenen.[31]
Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set 





D




{\displaystyle {\mathcal {D}}}

, the learner is also given a set
of test examples to be classified. Formally, a transductive support-vector machine is defined by the following primal optimization problem:[32]
Minimize (in 







w
→



,
b
,




y

⋆


→






{\displaystyle {{\vec {w}},b,{\vec {y^{\star }}}}}

)
subject to (for any 



i
=
1
,
…
,
n


{\displaystyle i=1,\dots ,n}

 and any 



j
=
1
,
…
,
k


{\displaystyle j=1,\dots ,k}

)
and
Transductive support-vector machines were introduced by Vladimir N. Vapnik in 1998.
SVMs have been generalized to structured SVMs, where the label space is structured and of possibly infinite size.
A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola.[33] This method is called support-vector regression (SVR). The model produced by support-vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support-vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.[34]
Training the original SVR means solving[35]
where 




x

i




{\displaystyle x_{i}}

 is a training sample with target value 




y

i




{\displaystyle y_{i}}

. The inner product plus intercept 



⟨
w
,

x

i


⟩
+
b


{\displaystyle \langle w,x_{i}\rangle +b}

 is the prediction for that sample, and 



ε


{\displaystyle \varepsilon }

 is a free parameter that serves as a threshold: all predictions have to be within an 



ε


{\displaystyle \varepsilon }

 range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.
In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation.[36] In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Wenzel et al. enabling the application of Bayesian SVMs to big data.[37] Wenzel et al. develop two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.[38]
The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.[39]
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.[40]
The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS[41]) and coordinate descent (e.g., LIBLINEAR[42]). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM[43]), especially when parallelization is allowed.
Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.
"
https://en.wikipedia.org/wiki/Regression_analysis,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared distances between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).
Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.[2][3]
The earliest form of regression was the method of least squares, which was published by Legendre in 1805,[4] and by Gauss in 1809.[5] Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821,[6] including a version of the Gauss–Markov theorem.
The term ""regression"" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).[7][8]
For Galton, regression had only this biological meaning,[9][10] but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.[11][12] In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. This assumption was weakened by R.A. Fisher in his works of 1922 and 1925.[13][14][15] Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.
In the 1950s and 1960s, economists used electromechanical desk ""calculators"" to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.[16]
Regression methods continue to be an area of active research. In recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, Bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.
In practice, researchers first select a model they would like to estimate and then use their chosen method (e.g., ordinary least squares) to estimate the parameters of that model. Regression models involve the following components:
In various fields of application, different terminologies are used in place of dependent and independent variables.
Most regression models propose that 




Y

i




{\displaystyle Y_{i}}

 is a function of 




X

i




{\displaystyle X_{i}}

 and 



β


{\displaystyle \beta }

, with 




e

i




{\displaystyle e_{i}}

 representing an additive error term that may stand in for un-modeled determinants of 




Y

i




{\displaystyle Y_{i}}

 or random statistical noise:
The researchers' goal is to estimate the function 



f
(

X

i


,
β
)


{\displaystyle f(X_{i},\beta )}

 that most closely fits the data. To carry out regression analysis, the form of the function 



f


{\displaystyle f}

 must be specified. Sometimes the form of this function is based on knowledge about the relationship between 




Y

i




{\displaystyle Y_{i}}

 and 




X

i




{\displaystyle X_{i}}

 that does not rely on the data. If no such knowledge is available, a flexible or convenient form for 



f


{\displaystyle f}

 is chosen. For example, a simple univariate regression may propose 



f
(

X

i


,
β
)
=

β

0


+

β

1



X

i




{\displaystyle f(X_{i},\beta )=\beta _{0}+\beta _{1}X_{i}}

, suggesting that the researcher believes 




Y

i


=

β

0


+

β

1



X

i


+

e

i




{\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{i}+e_{i}}

 to be a reasonable approximation for the statistical process generating the data.  
Once researchers determine their preferred statistical model, different forms of regression analysis provide tools to estimate the parameters 



β


{\displaystyle \beta }

. For example,  least squares (including its most common variant, ordinary least squares) finds the value of 



β


{\displaystyle \beta }

 that minimizes the sum of squared errors 




∑

i


(

Y

i


−
f
(

X

i


,
β
)

)

2




{\displaystyle \sum _{i}(Y_{i}-f(X_{i},\beta ))^{2}}

. A given regression method will ultimately provide an estimate of 



β


{\displaystyle \beta }

, usually denoted 






β
^





{\displaystyle {\hat {\beta }}}

 to distinguish the estimate from the true (unknown) parameter value that generated the data. Using this estimate, the researcher can then use the fitted value 







Y

i


^



=
f
(

X

i


,



β
^



)


{\displaystyle {\hat {Y_{i}}}=f(X_{i},{\hat {\beta }})}

 for prediction or to assess the accuracy of the model in explaining the data. Whether the researcher is intrinsically interested in the estimate 






β
^





{\displaystyle {\hat {\beta }}}

 or the predicted value 







Y

i


^





{\displaystyle {\hat {Y_{i}}}}

 will depend on context and her goals. As described in ordinary least squares, least squares is widely used because the estimated function 



f
(

X

i


,



β
^



)


{\displaystyle f(X_{i},{\hat {\beta }})}

 approximates the conditional expectation 



E
(

Y

i



|


X

i


)


{\displaystyle E(Y_{i}|X_{i})}

.[5] However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when researchers want to model other functions 



f
(

X

i


,
β
)


{\displaystyle f(X_{i},\beta )}

. 
It is important to note that there must be sufficient data to estimate a regression model. For example, suppose that a researcher has access to 



N


{\displaystyle N}

 rows of data with one dependent and two independent variables: 



(

Y

i


,

X

1
i


,

X

2
i


)


{\displaystyle (Y_{i},X_{1i},X_{2i})}

. Suppose further that the researcher wants to estimate a bivariate linear model via least squares: 




Y

i


=

β

0


+

β

1



X

1
i


+

β

2



X

2
i


+

e

i




{\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{1i}+\beta _{2}X_{2i}+e_{i}}

. If the researcher only has access to 



N
=
2


{\displaystyle N=2}

 data points, then they could find infinitely many combinations 



(




β
^




0


,




β
^




1


,




β
^




2


)


{\displaystyle ({\hat {\beta }}_{0},{\hat {\beta }}_{1},{\hat {\beta }}_{2})}

 that explain the data equally well: any combination can be chosen that satisfies 







Y
^




i


=




β
^




0


+




β
^




1



X

1
i


+




β
^




2



X

2
i




{\displaystyle {\hat {Y}}_{i}={\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}}

, all of which lead to 




∑

i






e
^




i


2


=

∑

i


(




Y
^




i


−
(




β
^




0


+




β
^




1



X

1
i


+




β
^




2



X

2
i


)

)

2


=
0


{\displaystyle \sum _{i}{\hat {e}}_{i}^{2}=\sum _{i}({\hat {Y}}_{i}-({\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}))^{2}=0}

 and are therefore valid solutions that minimize the sum of squared residuals. To understand why there are infinitely many options, note that the system of 



N
=
2


{\displaystyle N=2}

 equations is to be solved for 3 unknowns, which makes the system underdetermined. Alternatively, one can visualize infinitely many 3-dimensional planes that go through 



N
=
2


{\displaystyle N=2}

 fixed points. 
More generally, to estimate a least squares model with 



k


{\displaystyle k}

 distinct parameters, one must have 



N
≥
k


{\displaystyle N\geq k}

 distinct data points. If 



N
>
k


{\displaystyle N>k}

, then there does not generally exist a set of parameters that will perfectly fit the data. The quantity 



N
−
k


{\displaystyle N-k}

 appears often in regression analysis, and is referred to as the degrees of freedom in the model. Moreover, to estimate a least squares model, the independent variables 



(

X

1
i


,

X

2
i


,
.
.
.
,

X

k
i


)


{\displaystyle (X_{1i},X_{2i},...,X_{ki})}

 must be linearly independent: one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables. As discussed in ordinary least squares, this condition ensures that 




X

T


X


{\displaystyle X^{T}X}

 is an Invertible matrix and therefore that a solution 






β
^





{\displaystyle {\hat {\beta }}}

 exists.
By itself, a regression is simply a calculation using the data. In order to interpret the output of a regression as a meaningful statistical quantity that measures real-world relationships, researchers often rely on a number of classical assumptions. These often include:
A handful of conditions are sufficient for the least-squares estimator to possess desirable properties: in particular, the Gauss–Markov assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. Practitioners have developed a variety of methods to maintain some or all of these desirable properties in real-world settings, because these classical assumptions are unlikely to hold exactly. For example, modeling errors-in-variables can lead to reasonable estimates independent variables are measured with errors. Heteroscedasticity-consistent standard errors allow the variance of 




e

i




{\displaystyle e_{i}}

 to change across values of 




X

i




{\displaystyle X_{i}}

. Correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors, geographic weighted regression, or Newey–West standard errors, among other techniques. When rows of data correspond to locations in space, the choice of how to model 




e

i




{\displaystyle e_{i}}

 within geographic units can have important consequences.[17][18] The subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real-world conclusions in real-world settings, where classical assumptions do not hold exactly.
In linear regression, the model specification is that the dependent variable, 




y

i




{\displaystyle y_{i}}

 is a linear combination of the parameters (but need not be linear in the independent variables). For example, in simple linear regression for modeling 



n


{\displaystyle n}

 data points there is one independent variable: 




x

i




{\displaystyle x_{i}}

, and two parameters, 




β

0




{\displaystyle \beta _{0}}

 and 




β

1




{\displaystyle \beta _{1}}

:
In multiple linear regression, there are several independent variables or functions of independent variables.
Adding a term in 




x

i


2




{\displaystyle x_{i}^{2}}

 to the preceding regression gives:
This is still linear regression; although the expression on the right hand side is quadratic in the independent variable 




x

i




{\displaystyle x_{i}}

, it is linear in the parameters 




β

0




{\displaystyle \beta _{0}}

, 




β

1




{\displaystyle \beta _{1}}

 and 




β

2


.


{\displaystyle \beta _{2}.}


In both cases, 




ε

i




{\displaystyle \varepsilon _{i}}

 is an error term and the subscript 



i


{\displaystyle i}

 indexes a particular observation.
Returning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:
The residual, 




e

i


=

y

i


−




y
^




i




{\displaystyle e_{i}=y_{i}-{\widehat {y}}_{i}}

, is the difference between the value of the dependent variable predicted by the model, 







y
^




i




{\displaystyle {\widehat {y}}_{i}}

, and the true value of the dependent variable, 




y

i




{\displaystyle y_{i}}

. One method of estimation is ordinary least squares. This method obtains parameter estimates that minimize the sum of squared residuals, SSR:
Minimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, 







β
^




0


,




β
^




1




{\displaystyle {\widehat {\beta }}_{0},{\widehat {\beta }}_{1}}

.
In the case of simple regression, the formulas for the least squares estimates are
where 






x
¯





{\displaystyle {\bar {x}}}

 is the mean (average) of the 



x


{\displaystyle x}

 values and 






y
¯





{\displaystyle {\bar {y}}}

 is the mean of the 



y


{\displaystyle y}

 values.
Under the assumption that the population error term has a constant variance, the estimate of that variance is given by:
This is called the mean square error (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, 



(
n
−
p
)


{\displaystyle (n-p)}

 for 



p


{\displaystyle p}

 regressors or 



(
n
−
p
−
1
)


{\displaystyle (n-p-1)}

 if an intercept is used.[19] In this case, 



p
=
1


{\displaystyle p=1}

 so the denominator is 



n
−
2


{\displaystyle n-2}

.
The standard errors of the parameter estimates are given by
Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.
In the more general multiple regression model, there are 



p


{\displaystyle p}

 independent variables:
where 




x

i
j




{\displaystyle x_{ij}}

 is the 



i


{\displaystyle i}

-th observation on the 



j


{\displaystyle j}

-th independent variable.
If the first independent variable takes the value 1 for all 



i


{\displaystyle i}

, 




x

i
1


=
1


{\displaystyle x_{i1}=1}

, then 




β

1




{\displaystyle \beta _{1}}

 is called the regression intercept.
The least squares parameter estimates are obtained from 



p


{\displaystyle p}

 normal equations. The residual can be written as
The normal equations are
In matrix notation, the normal equations are written as
where the 



i
j


{\displaystyle ij}

 element of 




X



{\displaystyle \mathbf {X} }

 is 




x

i
j




{\displaystyle x_{ij}}

, the 



i


{\displaystyle i}

 element of the column vector 



Y


{\displaystyle Y}

 is 




y

i




{\displaystyle y_{i}}

, and the 



j


{\displaystyle j}

 element of 






β
^





{\displaystyle {\hat {\boldsymbol {\beta }}}}

 is 







β
^




j




{\displaystyle {\hat {\beta }}_{j}}

. Thus 




X



{\displaystyle \mathbf {X} }

 is 



n
×
p


{\displaystyle n\times p}

, 



Y


{\displaystyle Y}

 is 



n
×
1


{\displaystyle n\times 1}

, and 






β
^





{\displaystyle {\hat {\boldsymbol {\beta }}}}

 is 



p
×
1


{\displaystyle p\times 1}

. The solution is
Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.
Interpretations of these diagnostic tests rest heavily on the model's assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.
Limited dependent variables, which are response variables that are categorical variables or are variables constrained to fall only in a certain range, often arise in econometrics.
The response variable may be non-continuous (""limited"" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. Nonlinear models for binary dependent variables include the probit and logit model. The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For categorical variables with more than two values there is the multinomial logit. For ordinal variables with more than two values, there are the ordered logit and ordered probit models. Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used.
When the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in Differences between linear and non-linear least squares.
Regression models predict a value of the Y variable given known values of the X variables. Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction outside this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.
It is generally advised[citation needed] that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.
For such reasons and others, some tend to say that it might be unwise to undertake extrapolation.[21]
However, this does not cover the full set of modeling errors that may be made: in particular, the assumption of a particular form for the relation between Y and X. A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. Best-practice advice here[citation needed] is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is ""realistic"" (or in accord with what is known).
There are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One rule of thumb conjectured by Good and Hardin is 



N
=

m

n




{\displaystyle N=m^{n}}

, where 



N


{\displaystyle N}

 is the sample size, 



n


{\displaystyle n}

 is the number of independent variables and 



m


{\displaystyle m}

 is the number of observations needed to reach the desired precision if the model had only one independent variable.[22] For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (



N


{\displaystyle N}

). If the researcher decides that five observations are needed to precisely define a straight line (



m


{\displaystyle m}

), then the maximum number of independent variables the model can support is 4, because
Although the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:
All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.
"
https://en.wikipedia.org/wiki/Bayesian_network,"A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 



m


{\displaystyle m}

 parent nodes represent 



m


{\displaystyle m}

 Boolean variables, then the probability function could be represented by a table of 




2

m




{\displaystyle 2^{m}}

 entries, one entry for each of the 




2

m




{\displaystyle 2^{m}}

 possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.
Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
The joint probability function is:
where G = ""Grass wet (true/false)"", S = ""Sprinkler turned on (true/false)"", and R = ""Raining (true/false)"".
The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:
Using the expansion for the joint probability function 



Pr
(
G
,
S
,
R
)


{\displaystyle \Pr(G,S,R)}

 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,
Then the numerical results (subscripted by the associated variable values) are
To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function
obtained by removing the factor 



Pr
(
G
∣
S
,
R
)


{\displaystyle \Pr(G\mid S,R)}

 from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:
To predict the impact of turning the sprinkler on:
with the term 



Pr
(
S
=
T
∣
R
)


{\displaystyle \Pr(S=T\mid R)}

 removed, showing that the action affects the grass but not the rain.
These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 




do

(
x
)


{\displaystyle {\text{do}}(x)}

 can still be predicted, however, whenever the back-door criterion is satisfied.[1][2] It states that, if a set Z of nodes can be observed that d-separates[3] (or blocks) all back-door paths from X to Y then
A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible."" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not ""identified"". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of ""do-calculus""[1][4] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[5]
Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 




2

10


=
1024


{\displaystyle 2^{10}=1024}

 values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 



10
⋅

2

3


=
80


{\displaystyle 10\cdot 2^{3}=80}

 values.
One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.
Bayesian networks perform three main inference tasks:
Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.
In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)
Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions this process converges on maximum likelihood (or maximum posterior) values for parameters.
A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.
In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications the task of defining the network is too complex for humans. In this case the network structure and the parameters of the local distributions must be learned from data.
Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[6] and rests on the distinction between the three possible patterns allowed in a 3-node DAG:
The first 2 represent the same dependencies (



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are independent given 



Y


{\displaystyle Y}

) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[1][7][8][9]
An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[10][11] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[12] Such method can handle problems with up to 100 variables.
In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[13]
Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[14]
Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[15]
Given data 



x




{\displaystyle x\,\!}

 and parameter 



θ


{\displaystyle \theta }

, a simple Bayesian analysis starts with a prior probability (prior) 



p
(
θ
)


{\displaystyle p(\theta )}

 and likelihood 



p
(
x
∣
θ
)


{\displaystyle p(x\mid \theta )}

 to compute a posterior probability 



p
(
θ
∣
x
)
∝
p
(
x
∣
θ
)
p
(
θ
)


{\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}

.
Often the prior on 



θ


{\displaystyle \theta }

 depends in turn on other parameters 



φ


{\displaystyle \varphi }

 that are not mentioned in the likelihood. So, the prior 



p
(
θ
)


{\displaystyle p(\theta )}

 must be replaced by a likelihood 



p
(
θ
∣
φ
)


{\displaystyle p(\theta \mid \varphi )}

, and a prior 



p
(
φ
)


{\displaystyle p(\varphi )}

 on the newly introduced parameters 



φ


{\displaystyle \varphi }

 is required, resulting in a posterior probability
This is the simplest example of a hierarchical Bayes model.[clarification needed]
The process may be repeated; for example, the parameters 



φ


{\displaystyle \varphi }

 may depend in turn on additional parameters 



ψ




{\displaystyle \psi \,\!}

, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.
Given the measured quantities 




x

1


,
…
,

x

n






{\displaystyle x_{1},\dots ,x_{n}\,\!}

each with normally distributed errors of known standard deviation 



σ




{\displaystyle \sigma \,\!}

,
Suppose we are interested in estimating the 




θ

i




{\displaystyle \theta _{i}}

. An approach would be to estimate the 




θ

i




{\displaystyle \theta _{i}}

 using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply
However, if the quantities are related, so that for example the individual 




θ

i




{\displaystyle \theta _{i}}

have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,
with improper priors 



φ
∼


{\displaystyle \varphi \sim }

flat, 



τ
∼


{\displaystyle \tau \sim }

flat



∈
(
0
,
∞
)


{\displaystyle \in (0,\infty )}

. When 



n
≥
3


{\displaystyle n\geq 3}

, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual 




θ

i




{\displaystyle \theta _{i}}

 will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.
Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable 



τ




{\displaystyle \tau \,\!}

 in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.
Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.
X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[16]
where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).
For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[16]
Using the definition above, this can be written as:
The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.
X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[17]
where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v.
This can be expressed in terms similar to the first definition, as
The set of parents is a subset of the set of non-descendants because the graph is acyclic.
Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[18]
The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[17]
This definition can be made more general by defining the ""d""-separation of two nodes, where d stands for directional.[19][20] We first define the ""d""-separation of a trail and then we will define the ""d""-separation of two nodes in terms of that. Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:
The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.
X is a Bayesian network with respect to G if, for any two nodes u, v:
where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)
Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:
are equivalent: that is they impose exactly the same conditional independence requirements.
A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[1] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.
In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[21] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Dagum and Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[22] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ< 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF) and that approximate inference within a factor 2n1-ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[23][24]
In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[25] was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.
Notable software for Bayesian networks include:
The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[27]
In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[29] and Neapolitan's Probabilistic Reasoning in Expert Systems[30] summarized their properties and established them as a field of study.
"
https://en.wikipedia.org/wiki/Genetic_algorithm,"
In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by   the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection.[1] John Holland introduced genetic algorithms in 1960 based on the concept of Darwin’s theory of evolution; his student David E. Goldberg further extended GA in 1989.[2]
In a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.[3]
The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.
A typical genetic algorithm requires:
A standard representation of each candidate solution is as an array of bits.[3] Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.
Once the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.
The population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space). Occasionally, the solutions may be ""seeded"" in areas where optimal solutions are likely to be found.
During each successive generation, a portion of the existing population is selected to breed a new generation. Individual solutions are selected through a fitness-based process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.
The fitness function is defined over the genetic representation and measures the quality of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.
In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.
The next step is to generate a second generation population of solutions from those selected through a combination of genetic operators: crossover (also called recombination), and mutation.
For each new solution to be produced, a pair of ""parent"" solutions is selected for breeding from the pool selected previously. By producing a ""child"" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its ""parents"". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.
Although reproduction methods that are based on the use of two parents are more ""biology inspired"", some research[4][5] suggests that more than two ""parents"" generate higher quality chromosomes.
These processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.
Opinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.
Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.[citation needed]
It is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift  (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed.
In addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The speciation heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.[6][7]
This generational process is repeated until a termination condition has been reached. Common terminating conditions are:
Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:
Goldberg describes the heuristic as follows:
Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.[9][10] Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.[11][12][13]
There are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:
The simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.
When bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so-called Hamming walls, in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.
Other approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.[16][17]
An expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome.[18] This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.
A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.[19]
Parallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.
Other variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.
Genetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of pc and pm, AGAs utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm),[20] the adjustment of pc and pm depends on the fitness values of the solutions. In CAGA (clustering-based adaptive genetic algorithm),[21] through the use of clustering analysis to judge the optimization states of the population, the adjustment of pc and pm depends on these optimization states.
It can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA[citation needed] while overcoming the lack of robustness of hill climbing.
This means that the rules of genetic variation may have a different meaning in the natural case. For instance – provided that steps are stored in consecutive order – crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency.[22]
A variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.
A number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA,[23] GEMGA[24] and LLGA.[25]
Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs[citation needed]. GAs have also been applied to engineering.[26] Genetic algorithms are often applied as an approach to solve global optimization problems.
As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).
Examples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector,[27] antennae designed to pick up radio signals in space,[28] walking methods for computer figures,[29] optimal design of aerodynamic bodies in complex flowfields [30]
In his Algorithm Design Manual, Skiena advises against genetic algorithms for any task:
[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.
[...]

I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to simulated annealing for your heuristic search voodoo needs.In 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution.[32] Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.[33][34]  His 1954 publication was not widely noticed. Starting in 1957,[35]  the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)[36] and Crosby (1973).[37] Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.[38] Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).[39]
Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,[40] artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies.[41][42][43][44]  Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes.[45] 
In 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote[46] about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995.[47] Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version.[48] Since the 1990s, MATLAB has built in three derivative-free optimization heuristic algorithms (simulated annealing, particle swarm optimization, genetic algorithm) and two direct search algorithms (simplex search, pattern search).[49]
Genetic algorithms are a sub-field:
Evolutionary algorithms is a sub-field of evolutionary computing.
Swarm intelligence is a sub-field of evolutionary computing.
Evolutionary computation is a sub-field of the metaheuristic methods.
Metaheuristic methods broadly fall within stochastic optimisation methods.
"
https://en.wikipedia.org/wiki/Federated_learning,"Federated learning (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples. This approach stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed.
Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus addressing critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, or pharmaceutics.
Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights of a deep neural network) between these local models at some frequency to generate a global model.
Federated learning algorithms may use a central server that orchestrates the different steps of the algorithm and acts as a reference clock, or they may be peer-to-peer, where no such central server exists. In the non peer-to-peer case, a federated learning process can be broken down in multiple rounds, each consisting of 4 general steps.
The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets,[1] as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are identically distributed and roughly have the same size. None of these hypotheses are made for federated learning; instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude.
To ensure good task performance of a final, central machine learning model, federated learning relies on an iterative process broken up into an atomic set of client-server interactions known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model.
In the methodology below, we use a central server for this aggregation, while local nodes perform local training depending on the central server's orders. However, other strategies lead to the same results without central servers, in a peer-to-peer approach, using gossip methodologies.[2]
A statistical model (e.g., linear regression, neural network, boosting) is chosen to be trained on local nodes and initialized. Nodes are activated and wait for the central server to give calculation tasks.
For multiple iterations of so-called federated learning rounds, the following steps are performed:[3]
A fraction of local nodes are selected to start training on local data. They all acquire the same current statistical model from the central server. Other nodes wait for the next federated round.
The central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g. for some batch updates of gradient descent).
Each node returns the locally learned incremental model updates to the central server. The central server aggregates all results and stores the new model. It also handles failures (e.g., connection lost with a node while training). The system returns to the selection phase.
When a pre-specified termination criterion (e.g. maximal number of rounds or local accuracies higher than some target) has been met, the central server orders the end of the iterative training process. The central server contains a robust model which was trained on multiple heterogeneous data sources.
The way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches: for instance no central orchestrating server, or stochastic communication.[4]
In particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to a several randomly-selected others,[5] which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost.
Once the topology of the node network is chosen, one can control different parameters of the federated learning process (in opposition to the machine learning model's own hyperparameters) to optimize learning :
Other model-dependent parameters can also be tinkered with, such as :
Those parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, bandwidth). For instance, stochastically choosing a limited fraction C of nodes for each iteration diminishes computing cost and may prevent overfitting, in the same way that stochastic gradient descent can reduce overfitting.
In this section, we follow the exposition of Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan and al. 2017.
To describe the federated strategies, let us introduce some notations:
Deep learning training mainly relies on variants of stochastic gradient descent, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.
Federated stochastic gradient descent[6] is the direct transposition of this algorithm to the federated setting, but by using a random fraction C of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.
Federative averaging (FedAvg)[7] is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged model's performance.
Federated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoid data communication, which can require significant resources before starting centralized machine learning.
Federated learning raises several statistical challenges :
The main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.
With federated learning, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy. Despite such protective measures, these parameters mays still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using differential privacy or secure aggregation.[9]
The generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the federated learning methodology can be adapted to generate two models at once in a multi-task learning framework.
In the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will remain on each local node and only be trained on the local node's dataset.
Western legal frameworks emphasize more and more on data protection and data traceability. White House 2012 Report[10] recommended the application of a data minimization principle, which is mentioned in European GDPR.[11] In some cases, it is impossible to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases federated learning brings solutions to train a global model while respecting security constraints.
Federated learning has started to emerge as an important research topic in 2015[1] and 2016,[12] with the first publications on federative averaging in telecommunication settings. Recent publications have emphasized the development of resource allocation strategies, especially to reduce communication[13] requirements[14] between nodes with gossip algorithms.[15] In addition, recent publications continue to work on the federated algorithms robustness to differential privacy attacks.[16]
Federated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with other (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node.
One of the first use cases of federated learning was implemented by Google[3][17] for predictive keyboards. Under high regulatory pressure, it showed impossible to upload every user's text message to train the predictive algorithm for word guessing. Besides, such a process would hijack too much of the user's data. Despite the sometimes limited memory and computing power of smartphones, Google has made a compelling use case out of its G-board, as presented during the Google IO 2019 event.[18]
Pharmaceutical research is pivoting towards a new paradigm : real world data use for generating drug leads and synthetic control arms. Generating knowledge on complex biological problems require to gather a lot of data from diverse medical institutions, which are eager to maintain control of their sensitive patient data. Federated learning, especially assisted by high traceability technologies (distributive ledgers) enable researchers to train predictive models on many sensitive data in a transparent way without uploading them. In 2019, French start-up Owkin is pioneering the development of biomedical machine learning models based on such algorithms to capture heterogeneous data from both pharmaceutical companies and medical institutions.
Self driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes.
Some libraries have been made to facilitate the development of federated learning systems, including: 
"
https://en.wikipedia.org/wiki/Algorithmic_bias,"
Algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect ""systematic and unfair"" discrimination. This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation.
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias stem from the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single ""algorithm"" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.
Algorithms are difficult to define,[2] but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.[3]:13 For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence.[4]:14–15 By analyzing and processing data, algorithms are the backbone of search engines,[5] social media websites,[6] recommendation engines,[7] online retail,[8] online advertising,[9] and more.[10]
Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.[11]:2[12]:563[13]:294[14] The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased.[15]:332 This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).
Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria.[16]:3 Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.[16]:4 Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.[16]:8 Other algorithms may reinforce stereotypes and preferences as they process and display ""relevant"" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.[16]:6
Beyond assembling and processing data, bias can emerge as a result of design.[17] For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).[18]:36 Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as in flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths.[17] Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.[19]:4
The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, Artificial Intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.[20]:149
Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs ""embody law,""[20]:40 that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including his or her biases and expectations.[20]:109 While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects ""human decisionmaking processes"" as data is being selected.[20]:70, 105
Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results.[20]:65 Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.[20]:226
An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with ""foreign-sounding names"" based on historical trends in admissions.[22]
Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze.[23] The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.[24]:115 In theory, these biases may create new patterns of behavior, or ""scripts,"" in relationship to specific technologies as the code interacts with other elements of society.[25] Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.[26]:180
The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,[27]:15 a process described by author Clay Shirky as ""algorithmic authority"".[28] Shirky uses the term to describe ""the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,"" such as search results.[28] This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as ""trending"" or ""popular"" may be created based on significantly wider criteria than just their popularity.[16]:14
Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.[27]:16[29]:6 This can have the effect of reducing alternative options, compromises, or flexibility.[27]:16 Sociologist Scott Lash has critiqued algorithms as a new form of ""generative power"", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.[30]:71
Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,
and Transparency in Machine Learning.[31]:115 Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.[31]:117 In recent years, the study of the Fairness, Accountability,
and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAT*.[32]
However, FAT has come under serious criticism itself due to dubious funding.[33]
Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.[15]:334[13]:294 Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.[21]:17 Encoding pre-existing bias into software can preserve social and institutional bias, and without correction, could be replicated in all future uses of that algorithm.[24]:116[29]:8
An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act.[15]:341 The program accurately reflected the tenets of the law, which stated that ""a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.""[15]:341[34]:375 In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.[15]:342
Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.[15]:332 Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.[15]:336 Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.[15]:332
A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.[15]:332 The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.[12]:574
Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.[15]:332 Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.[27]:21–22
Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.[15]:334 Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.[15]:334,336 This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.[26]:179[13]:294 Similarly, problems may emerge when training data (the samples ""fed"" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.[35]
In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).[15]:338 The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.[15]:338[36]
Additional emergent biases include:
Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.[19]:6 In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.[37]
Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.[15]:334 These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.[26]:179
Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of UK immigration law.[15]:342
Emergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm.[38][39] For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public.[40] The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods.[38][41][42] The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.[39]
Recommender systems such as those used to recommend online videos or news articles can create feedback loops.[43] When users click on content that is suggested by algorithms, it influences the next set of suggestions.[44] Over time this may lead to users entering a Filter Bubble and being unaware of important or useful content.[45][46]
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.[47]:2[15]:331
In a 1998 paper describing Google, it was shown that the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that ""advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.""[48] This bias would be an ""invisible"" manipulation of the user.[47]:3
A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have ""no means of competing"" if an algorithm, with or without intent, boosted page listings for a rival candidate.[49] Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted.[50] Legal scholar Jonathan Zittrain has warned that this could create a ""digital gerrymandering"" effect in elections, ""the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users"", if intentionally manipulated.[51]:335
In 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, ""Andrea"" would bring up a prompt asking if users meant ""Andrew,"" but queries for ""Andrew"" did not ask if users meant to find ""Andrea"". The company said this was the result of an analysis of users' interactions with the site.[52]
In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners.[53]:94[54] Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.[53]:98
Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, ""lesbian"". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, ""Top 25 Sexiest Women Athletes"" articles displayed as first-page results in searches for ""women athletes"".[55]:31 In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.[56] Other examples include the display of higher-paying jobs to male applicants on job search websites.[57] Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations[58]. In fact, current machine translation systems fail to reproduce the real world distribution of female workers. 
In 2018, Amazon.com turned off a system it developed to screen job applications when they realized it was biased against women.[59]
Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.[60]:158 Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime.[61][62] This could potentially mean that a system amplifies the original biases in the data.
One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime.[63] For the time period starting in 1920 and ending in 1970, the nationality of a criminals's father was a consideration in those risk assessment scores.[64]:4 Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.[63]
In 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas.[65] In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking.[66] Such examples are the product of bias in biometric data sets.[65] Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.[60]:154 Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.[67]
Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.[68]
One study that set out to examine “Risk, Race, & Recidivism: Predictive Bias and Disparate Impact” alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.[69]
In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.[70]
A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on ""creditworthiness"" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.[71][non-primary source needed]
In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents.[72] The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing ""Muslims"" would be blocked, while posts denouncing ""Radical Muslims"" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the ""children"" subset of blacks, rather than ""all blacks,"" whereas ""all white men"" would trigger a block, because whites and males are not considered subsets.[72] Facebook was also found to allow ad purchasers to target ""Jew haters"" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.[73]
Although algorithms are a great way to flag hate speech, there has been a racial bias noticed from the algorithms. Algorithms were found to be 1 and a half more likely to flag information if it was posted by an African American person and 2.2 times likely to flag information as hate speech if written in Ebonics. The study was completed at the University of Washington in the year of 2019.[74] The researchers looked at flagged hate speech on Twitter. There are slurs and epithets that communities have re-appropriated, and people within that community aren't using offensively, such as ""queer"" and the ""N-word"", but algorithms don't understand the context in which these words are being used and will flag the content anyway[75].  
Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.[12]:572 The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender.[76] A 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.[26]:190 Additional studies of facial recognition software have found the opposite to be true when trained on non-criminal databases, with the software being the least accurate in identifying darker-skinned females.[77]
In 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men.[78] In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its ""adult content"" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain.[79][16]:5[80]
In 2019, it was found that on Facebook, searches for ""photos of my female friends"" yielded suggestions such as ""in bikinis"" or ""at the beach"". In contrast, searches for ""photos of my male friends"" yielded no results.[81]
Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning[82]. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, a instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.[83]
There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images.[84] The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being ""outed"" against their will.[85]
While users generate results that are ""completed"" automatically, Google has failed to remove sexist and racist autocompletion text. In Algorithms of Oppression: How Search Engines Reinforce Racism, Safiya Noble notes an example of the search for ""black girls,"" which was reported to result in pornographic images. Due to Google's algorithm, it is unable to erase pages unless they qualify as unlawful.[86]
Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.[11]:5
Algorithmic processes are complex, often exceeding the understanding of the people who use them.[11]:2[87]:7 Large-scale operations may not be understood even by those involved in creating them.[88] The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.[26]:183
Social scientist Bruno Latour has identified this process as blackboxing, a process in which ""scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.""[89] Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.[90]:92
An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013.[91] Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.[24]:118 Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.[92]:22
Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.[93]:367[87]:7 One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.[11]:5
Companies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.[11]:5
Commercial algorithms are proprietary, and may be treated as trade secrets.[11]:2[87]:7[26]:183 Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings.[93]:366 This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.[92]:20 Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.[93]:369 Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.[94]
A significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data.[95] In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.
Some practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation, for example building systems to infer ethnicity from names,[96] however this can introduce other forms of bias if not undertaken with care.[97] Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.[98]
Algorithmic bias does not only include protected categories, but can also concerns characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult.[99] Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.[100]
A study of 84 policy guidelines on ethical AI found that fairness and ""mitigation of unwanted bias"" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts.[101]
There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent field focuses on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion).[102][103][104][105][106][107][108][109][110]
Currently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.[111][112]
Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results.[113] Such solutions include the consideration of the ""right to understanding"" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed.[114] Toward this end, a movement for ""Explainable AI"" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias.[115] Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.[116]
From a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias.[117] This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes.[118] Others propose the need for clear liability insurance mechanisms.[119]
Amid concerns that the design of AI systems is primarily the domain of white, male engineers,[120] a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems.[114][101] For example, just 12% of machine learning engineers are women,[121] with black AI leaders pointing to a ""diversity crisis"" in the field.[122] Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms.[123]
The General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses ""Automated individual decision-making, including profiling"" in Article 22. These rules prohibit ""solely"" automated decisions which have a ""significant"" or ""legal"" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.[124]

The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71,[125] noting that ... the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals.[126] While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law,[125] its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.[127]
The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used.[128] Many policies are self-enforced or controlled by the Federal Trade Commission.[128] In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan,[129] which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to ""design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases"". Intended only as guidance, the report did not create any legal precedent.[130]:26
In 2017, New York City passed the first algorithmic accountability bill in the United States.[131] The bill, which went into effect on January 1, 2018, required ""the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.""[132] The task force is required to present findings and recommendations for further regulatory action in 2019.[133]
On July 31, 2018, a draft of the Personal Data Bill was presented.[134] The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for ""...harm resulting from any processing or any kind of processing undertaken by the fiduciary"". It defines ""any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal"" or ""any discriminatory treatment""  as a source of harm that could arise from improper use of data. It also makes special provisions for people of ""Intersex status”.[135]
"
